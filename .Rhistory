}
return(X)
}
MX(a, b, d, m, N)
MX_paths <- matrix(nrow = nsim, ncol = N + 1) # fill the matrix with random paths that follow
for (i in 1:nsim) { # the function MX just created
MX_paths[i, ] <- MX(a, b, d, m, N)
}
MX_paths %>%
t() %>%
format(x = , scientific = TRUE, digits = 3) %>%
datatable()
# plot the Monte Carlo Simulation
plot(MX_paths[1, ],
col = 0, type = "l", ylim = c(min(MX_paths), max(MX_paths)),
main = "Monte Carlo Simulation for Meixner returns", sub = "100 steps, 10 paths",
xlab = "Time", ylab = "MXNR returns"
)
for (i in 2:nsim) {
lines(MX_paths[i, ], col = colori[i], lwd = 2)
}
# QQplot
MX.q <- uq(pinvd.new(udmeixner(a, b, d, m)), p) # compute the quantile
plot(MX.q, l_ret.s,
main = "Meixner Q-Q Plot",
xlab = "Theoretical Quantiles", ylab = "Sample Quantiles"
)
# good result, linear
# summary statistics
final_retMX <- MX_paths[, N + 1]
basicStats(final_retMX)
hist(final_retMX) # not much disclosing when nsim is small
MX(a, b, d, m, N) %>% quickplot()
replicate(10,MX(a, b, d, m, N)) %>% quickplot()
replicate(10,MX(a, b, d, m, N)) %>% apply(2, delog) %>% quickplot()
MX_paths <- matrix(nrow = nsim, ncol = N + 1) # fill the matrix with random paths that follow
# MEIXNER MODEL
# Moments: mean, variance, skewness, kurtosis
x <- mean(l_ret, na.rm = TRUE)
y <- sd(l_ret, na.rm = TRUE)
z <- as.numeric(skewness(l_ret, na.rm = TRUE))
w <- as.numeric(kurtosis(l_ret, na.rm = TRUE))
# Mom: estimates parameters m, a, b, d as functions of the moments
m <- x - ((z * sqrt(y)) / (w - (z^2) - 3))
a <- sqrt(y * (2 * w - 3 * (z^2) - 6))
b <- 2 * atan(-sqrt((z^2) / (2 * w - 3 * (z^2) - 6)))
d <- 1 / (w - (z^2) - 3)
# risk neutral transformation
# Esscher transform: Meixner(a, a*theta + b, d, m) distribution
# theta <- -1/a * (b + 2 * atan((-cos(a/2)+ exp((m-r)/2*d))/sin(a/2)))
# b <- a*theta+b
# mean correction
# m <- r -2 *d*log(cos(b/2)/cos((a+b)/2))
# Meixner function
MX <- function(a, b, d, , N) {
distr <- udmeixner(a, b, d, m) # meixner distribution
gen <- pinvd.new(distr) # Polynomial interpolation of INVerse CDF
rdmMXgen <- ur(gen, N) # randomly draws N objects from gen (from a Meixner distr)
h <- Time / N
X <- rep(0, N + 1)
for (i in 1:N) {
X[i + 1] <- X[1] + rdmMXgen[i]
}
return(X)
}
replicate(10,MX(a, b, d, m, N)) %>% apply(2, delog) %>% quickplot()
MX_paths <- matrix(nrow = nsim, ncol = N + 1) # fill the matrix with random paths that follow
for (i in 1:nsim) { # the function MX just created
MX_paths[i, ] <- MX(a, b, d, m, N)
}
MX_paths %>%
t() %>%
format(x = , scientific = TRUE, digits = 3) %>%
datatable()
# plot the Monte Carlo Simulation
plot(MX_paths[1, ],
col = 0, type = "l", ylim = c(min(MX_paths), max(MX_paths)),
main = "Monte Carlo Simulation for Meixner returns", sub = "100 steps, 10 paths",
xlab = "Time", ylab = "MXNR returns"
)
for (i in 2:nsim) {
lines(MX_paths[i, ], col = colori[i], lwd = 2)
}
# QQplot
MX.q <- uq(pinvd.new(udmeixner(a, b, d, m)), p) # compute the quantile
plot(MX.q, l_ret.s,
main = "Meixner Q-Q Plot",
xlab = "Theoretical Quantiles", ylab = "Sample Quantiles"
)
# good result, linear
# summary statistics
final_retMX <- MX_paths[, N + 1]
basicStats(final_retMX)
hist(final_retMX) # not much disclosing when nsim is small
udmeixner(a, b, d, m)
p
MX_paths
t(MX_paths) %>% quickplot()
t(MX_paths) %>% quickplot(show_legend = F)
# library(quantmod)
# library(ggplot2)
# library(magrittr)
library(broom)
# library(dplyr)
# library(plotly)
library(hrbrthemes)
library("viridis")
# library(sde)
library(tseries)
library(Runuran)
library(moments)
library(fBasics)
library("stats4")
library(yuima)
library(VarianceGamma)
library(ghyp)
library(GeneralizedHyperbolic)
library(EnvStats)
library(Sim.DiffProc)
source("C:/Users/pietr/OneDrive/Desktop/formula.main.R")
knitr::opts_chunk$set(error=TRUE)
knitr::opts_chunk$set(warning = FALSE)
Sys.setlocale("LC_TIME", "English") # set output language in English
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(tidy=TRUE)
library(microbenchmark)
gghistogram <- function (x, add.normal = FALSE, add.kde = FALSE, add.rug = FALSE, bins, boundary = 0,
fill = "#1f77b4")
{
if (!requireNamespace("ggplot2", quietly = TRUE)) {
stop("ggplot2 is needed for this function to work. Install it via install.packages(\"ggplot2\")",
call. = FALSE)
}
else {
if (missing(bins)) {
bins <- min(500, grDevices::nclass.FD(na.exclude(x)))
}
data <- data.frame(x = as.numeric(c(x)))
binwidth <- (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/bins
p <- ggplot2::ggplot() + ggplot2::geom_histogram(ggplot2::aes(x),
data = data, binwidth = binwidth, boundary = boundary, fill = fill) +
ggplot2::xlab(deparse(substitute(x)))
if (add.normal || add.kde) {
xmin <- min(x, na.rm = TRUE)
xmax <- max(x, na.rm = TRUE)
if (add.kde) {
h <- stats::bw.SJ(x)
xmin <- xmin - 3 * h
xmax <- xmax + 3 * h
}
if (add.normal) {
xmean <- mean(x, na.rm = TRUE)
xsd <- sd(x, na.rm = TRUE)
xmin <- min(xmin, xmean - 3 * xsd)
xmax <- max(xmax, xmean + 3 * xsd)
}
xgrid <- seq(xmin, xmax, length.out = 512)
if (add.normal) {
df <- data.frame(x = xgrid, y = length(x) * binwidth *
stats::dnorm(xgrid, xmean, xsd))
p <- p + ggplot2::geom_line(ggplot2::aes(df$x,
df$y), col = "#ff8a62")
}
if (add.kde) {
kde <- stats::density(x, bw = h, from = xgrid[1],
to = xgrid[512], n = 512)
p <- p + ggplot2::geom_line(ggplot2::aes(x = kde$x,
y = length(x) * binwidth * kde$y), col = "#67a9ff")
}
}
if (add.rug) {
p <- p + ggplot2::geom_rug(ggplot2::aes(x))
}
return(p)
}
}
startLV <- Qdate(9, 10, 2017)
S <- Cl(getSymbols("AAPL", from = startLV, to = "2024-02-16", auto.assign = F))
cp <- cpoint(as.numeric(S))
cp
ggplot(data = S, aes(x = index(S), y = S))+
geom_line()+
geom_vline(aes(xintercept = startLV+cp$tau0))
S <- Cl(getSymbols("AAPL", from = startLV+cp$tau0, to = "2024-02-16", auto.assign = F))
l_ret <- na.omit(diff(log(S)))
# VARIANCE-GAMMA MODEL
# Parameters fitting
vgfit <- vgFit(l_ret) # esitmate VG parameters on the sample
str(vgfit)
summary(vgfit)
data.frame(vgfit$param)
vg_param <- as.numeric(vgfit$param)
c <- vg_param[1]
sigma <- vg_param[2]
theta <- vg_param[3]
nu <- vg_param[4]
Time <- 1/4 # option maturity = 3 months
N <- 100    # number of steps for each path
r <- 0.01   # arbitrary risk-free rate
nsim <- 100  # number of simulated path
# Variance Gamma function
VG <- function(sigma, nu, theta, Tf, N, r) {
a <- 1 / nu
b <- 1 / nu
h <- Time / N
Time <- (0:N) * Time / N
X <- rep(0, N + 1)
I <- rep(0, N)
X[1] <- 0
for (i in 1:N) {
I[i] <- rgamma(1, a * h, b)
X[i + 1] <- X[i] + theta * I[i] + sigma * sqrt(I[i]) * rnorm(1)
}
return((X))
}
VG_paths <- matrix(nrow = nsim, ncol = N + 1) # fill the matrix with random paths that follow
for (i in 1:nsim) { # the function VG just created
VG_paths[i, ] <- VG(sigma, nu, theta, Time, N, r)
}
VG_paths %>%
t() %>%
format(scientific = TRUE, digits = 2) %>%
datatable()
# plot the Monte Carlo Simulation
colori <- viridis(nsim)
plot(VG_paths[1, ],
col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
main = "Monte Carlo Simulation for VG returns", sub = "100 steps, 10 paths",
xlab = "Time", ylab = "VG returns"
)
for (i in 2:nsim) {
lines(VG_paths[i, ], col = colori[i], lwd = 2)
}
grid.arrange(
ncol = 2,
cbind(VG_paths) %>% t() %>% quickplot(title = "Montecarlo simulations", show_legend = F),
quickplot(apply(VG_paths, 2, mean), title = "Mean across timesteps", show_legend = F)
)
## TESTS (both graphical and not) OF DISTRIBUTIONAL ASSUMPTIONS
# QQplot
l_ret.s <- sort(as.numeric(l_ret)) # sort the log returns
p <- ppoints(length(l_ret.s)) # plotting position
VG.q <- qvg(p, vgC = c, sigma = sigma, theta = theta, nu = nu) # compute the quantile
plot(VG.q, l_ret.s,
main = "Variance-Gamma Q-Q Plot",
xlab = "Theoretical Quantiles", ylab = "Sample Quantiles"
)
# good result, linear
ggplot(data.frame(VG.q, l_ret.s), aes(x = VG.q, y = l_ret.s))+
geom_point()
# Density comparison
# kernel density and VG overlayed (Gaussian kernel, Silverman's rule of thumb)
{plot(density(l_ret[-1, ]),
type = "l", lwd = 2, lty = 3, col = "coral2",
xlim = c(-0.03, 0.03), ylim = c(0, 120), main = "", xlab = "", ylab = ""
)
legend("topright",
inset = .02, c("Kernel", "VG"),
col = c("coral2", "seagreen3"), lwd = c(2, 1), lty = c(3, 1), cex = 0.8, bty = "n"
)
points(seq(min(l_ret[-1, ]), max(l_ret[-1, ]), length.out = 500),
dvg(
seq(min(l_ret[-1, ]), max(l_ret[-1, ]), length.out = 500),
mean(l_ret[-1, ]), sd(l_ret[-1, ])
),
type = "l", col = "seagreen3"
)# better fitting than the log-normal case
}
#Tests
#H0 = The data is consistent with a specified reference distribution.
#H1 = The data is NOT consistent with a specified reference distribution
#Chi^2 test
test <- chisq.test(l_ret.s, VG.q)
#high p-value (0.24) -> we can'Time reject the null hypotesis
#K-S test
ks.test(as.numeric(l_ret), rvg(length(as.numeric(l_ret)), param = c(c, sigma, theta, nu)))
#high p-value (0.10) -> we can'Time reject the null hypotesis
#summary statistics
final_retVG<-VG_paths[,N+1]
basicStats(final_retVG)
hist(final_retVG) #not much disclosing when nsim is small
gghistogram(final_retVG) #not much disclosing when nsim is small
gghistogram(final_retVG, bins = 80) #not much disclosing when nsim is small
gghistogram(final_retVG, bins = 30) #not much disclosing when nsim is small
gghistogram(final_retVG, bins = 30, add.normal = T) #not much disclosing when nsim is small
gghistogram(final_retVG, bins = 30, add.kde = T) #not much disclosing when nsim is small
gghistogram(final_retVG, bins = 30) #not much disclosing when nsim is small
if (!requireNamespace("ggplot2", quietly = TRUE)) {
stop("ggplot2 is needed for this function to work. Install it via install.packages(\"ggplot2\")",
call. = FALSE)
}
gghistogram <- function (x, add.normal = FALSE, add.kde = FALSE, add.rug = FALSE, bins, boundary = 0,
fill = "#1f77b4", title = element_blank(), subtitle = element_blank()) {
if (!requireNamespace("ggplot2", quietly = TRUE)) {
stop("ggplot2 is needed for this function to work. Install it via install.packages(\"ggplot2\")",
call. = FALSE)
}
else {
if (missing(bins)) {
bins <- min(500, grDevices::nclass.FD(na.exclude(x)))
}
data <- data.frame(x = as.numeric(c(x)))
binwidth <- (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/bins
p <- ggplot2::ggplot() +
ggplot2::geom_histogram(ggplot2::aes(x), data = data, binwidth = binwidth,
boundary = boundary, fill = fill) +
ggplot2::labs(title = title, subtitle = subtitle)
ggplot2::xlab(deparse(substitute(x)))
if (add.normal || add.kde) {
xmin <- min(x, na.rm = TRUE)
xmax <- max(x, na.rm = TRUE)
if (add.kde) {
h <- stats::bw.SJ(x)
xmin <- xmin - 3 * h
xmax <- xmax + 3 * h
}
if (add.normal) {
xmean <- mean(x, na.rm = TRUE)
xsd <- sd(x, na.rm = TRUE)
xmin <- min(xmin, xmean - 3 * xsd)
xmax <- max(xmax, xmean + 3 * xsd)
}
xgrid <- seq(xmin, xmax, length.out = 512)
if (add.normal) {
df <- data.frame(x = xgrid, y = length(x) * binwidth *
stats::dnorm(xgrid, xmean, xsd))
p <- p + ggplot2::geom_line(ggplot2::aes(df$x,
df$y), col = "#ff8a62")
}
if (add.kde) {
kde <- stats::density(x, bw = h, from = xgrid[1],
to = xgrid[512], n = 512)
p <- p + ggplot2::geom_line(ggplot2::aes(x = kde$x,
y = length(x) * binwidth * kde$y), col = "#67a9ff")
}
}
if (add.rug) {
p <- p + ggplot2::geom_rug(ggplot2::aes(x))
}
return(p)
}
}
gghistogram(final_retVG, bins = 30) #not much disclosing when nsim is small
gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps",
subtitle = "not much disclosing when nsim is small")
subtitle = paste("not much disclosing when nsim is small (", nsim,")")
gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps",
subtitle = paste("not much disclosing when nsim is small (", nsim,")"))
gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps",
subtitle = paste0("not much disclosing when nsim is small (", nsim,")"))
gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps",
subtitle = paste0("not much disclosing when nsim is small (now =", nsim,")"))
gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps",
subtitle = paste0("not much disclosing when nsim is small (now = ", nsim,")"))
desc_df(final_retVG)
final_retVG
desc_df(data.frame(final_retVG))
startLV <- Qdate(9, 10, 2017)
S <- Cl(getSymbols("AAPL", from = startLV, to = "2024-02-16", auto.assign = F))
cp <- cpoint(as.numeric(S))
cp
ggplot(data = S, aes(x = index(S), y = S))+
geom_line()+
geom_vline(aes(xintercept = startLV+cp$tau0))
S <- Cl(getSymbols("AAPL", from = startLV+cp$tau0, to = "2024-02-16", auto.assign = F))
l_ret <- na.omit(diff(log(S)))
# VARIANCE-GAMMA MODEL
# Parameters fitting
vgfit <- vgFit(l_ret) # esitmate VG parameters on the sample
str(vgfit)
summary(vgfit)
data.frame(vgfit$param)
vg_param <- as.numeric(vgfit$param)
c <- vg_param[1]
sigma <- vg_param[2]
theta <- vg_param[3]
nu <- vg_param[4]
Time <- 1/4 # option maturity = 3 months
N <- 100    # number of steps for each path
r <- 0.01   # arbitrary risk-free rate
nsim <- 100  # number of simulated path
# Variance Gamma function
VG <- function(sigma, nu, theta, Tf, N, r) {
a <- 1 / nu
b <- 1 / nu
h <- Time / N
Time <- (0:N) * Time / N
X <- rep(0, N + 1)
I <- rep(0, N)
X[1] <- 0
for (i in 1:N) {
I[i] <- rgamma(1, a * h, b)
X[i + 1] <- X[i] + theta * I[i] + sigma * sqrt(I[i]) * rnorm(1)
}
return((X))
}
VG_paths <- matrix(nrow = nsim, ncol = N + 1) # fill the matrix with random paths that follow
for (i in 1:nsim) { # the function VG just created
VG_paths[i, ] <- VG(sigma, nu, theta, Time, N, r)
}
VG_paths %>%
t() %>%
format(scientific = TRUE, digits = 2) %>%
datatable()
# plot the Monte Carlo Simulation
colori <- viridis(nsim)
plot(VG_paths[1, ],
col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
main = "Monte Carlo Simulation for VG returns", sub = "100 steps, 10 paths",
xlab = "Time", ylab = "VG returns"
)
for (i in 2:nsim) {
lines(VG_paths[i, ], col = colori[i], lwd = 2)
}
grid.arrange(
ncol = 2,
cbind(VG_paths) %>% t() %>% quickplot(title = "Montecarlo simulations", show_legend = F),
quickplot(apply(VG_paths, 2, mean), title = "Mean across timesteps", show_legend = F)
)
## TESTS (both graphical and not) OF DISTRIBUTIONAL ASSUMPTIONS
# QQplot
l_ret.s <- sort(as.numeric(l_ret)) # sort the log returns
p <- ppoints(length(l_ret.s)) # plotting position
VG.q <- qvg(p, vgC = c, sigma = sigma, theta = theta, nu = nu) # compute the quantile
plot(VG.q, l_ret.s,
main = "Variance-Gamma Q-Q Plot",
xlab = "Theoretical Quantiles", ylab = "Sample Quantiles"
)
# good result, linear
ggplot(data.frame(VG.q, l_ret.s), aes(x = VG.q, y = l_ret.s))+
geom_point()
# Density comparison
# kernel density and VG overlayed (Gaussian kernel, Silverman's rule of thumb)
{plot(density(l_ret[-1, ]),
type = "l", lwd = 2, lty = 3, col = "coral2",
xlim = c(-0.03, 0.03), ylim = c(0, 120), main = "", xlab = "", ylab = ""
)
legend("topright",
inset = .02, c("Kernel", "VG"),
col = c("coral2", "seagreen3"), lwd = c(2, 1), lty = c(3, 1), cex = 0.8, bty = "n"
)
points(seq(min(l_ret[-1, ]), max(l_ret[-1, ]), length.out = 500),
dvg(
seq(min(l_ret[-1, ]), max(l_ret[-1, ]), length.out = 500),
mean(l_ret[-1, ]), sd(l_ret[-1, ])
),
type = "l", col = "seagreen3"
)# better fitting than the log-normal case
}
# Tests
# H0 = The data is consistent with a specified reference distribution.
# H1 = The data is NOT consistent with a specified reference distribution
# Chi^2 test
test <- chisq.test(l_ret.s, VG.q)
# high p-value (0.24) -> we can'Time reject the null hypotesis
# K-S test
ks.test(as.numeric(l_ret), rvg(length(as.numeric(l_ret)), param = c(c, sigma, theta, nu)))
# high p-value (0.10) -> we can'Time reject the null hypotesis
# summary statistics
final_retVG <- VG_paths[, N + 1]
desc_df(data.frame(final_retVG))
gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps",
subtitle = paste0("not much disclosing when nsim is small (now = ", nsim,")"))
nsim
plot(VG_paths[1, ],
col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
main = "Monte Carlo Simulation for VG returns", sub = paste(N "steps", nsim, "paths"),
main = "Monte Carlo Simulation for VG returns", sub = paste(N "steps", nsim, "paths")),
plot(VG_paths[1, ],
col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
main = "Monte Carlo Simulation for VG returns", sub = paste(N "steps", nsim, "paths"),
paste(N "steps", nsim, "paths")
plot(VG_paths[1, ],
col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
main = "Monte Carlo Simulation for VG returns", sub = paste(N, "steps", nsim, "paths"),
xlab = "Time", ylab = "VG returns"
)
plot(VG_paths[1, ],
col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
main = "Monte Carlo Simulation for VG returns", sub = paste(N, "steps", nsim, "paths"),
xlab = "Time", ylab = "VG returns"
)
for (i in 2:nsim) {
lines(VG_paths[i, ], col = colori[i], lwd = 2)
}
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
year(Sys.Date())
Sys.Date()
month(Sys.Date()) * 30
month(Sys.Date()) * 30 - day(Sys.Date())
Sys.Date() - month(Sys.Date()) * 30 - day(Sys.Date())
Sys.Date() - month(Sys.Date()) * 30.4375 - day(Sys.Date())
Sys.Date() - (month(Sys.Date())-1) * 30.4375 - day(Sys.Date())
Sys.Date() - (month(Sys.Date())-1) * 30.4375 - day(Sys.Date())+1
Sys.Date() - (month(Sys.Date())-1) * 30.4375 - day(Sys.Date()+1)
Sys.Date() - (month(Sys.Date())-1) * 30.4375 - day(Sys.Date())
day(Sys.Date())
month(Sys.Date())) * 30.4375 - day(Sys.Date())
month(Sys.Date()) * 30.4375 - day(Sys.Date())
Sys.Date-month(Sys.Date()) * 30.4375 - day(Sys.Date())
Sys.Date()-month(Sys.Date()) * 30.4375 - day(Sys.Date())
Sys.Date()-month(Sys.Date()+1) * 30.4375 - day(Sys.Date())
Sys.Date()-month(Sys.Date())+1 * 30.4375 - day(Sys.Date())
Sys.Date()-(month(Sys.Date())+1) * 30.4375 - day(Sys.Date())
Sys.Date()-(month(Sys.Date())-1) * 30.4375 - day(Sys.Date())
month(Sys.Date())-1
original_tickers
q()
