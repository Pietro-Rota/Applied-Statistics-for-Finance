---
title: "Applied Statistics for Finance"
date-modified: "`r Sys.Date()`"
editor: source
format:
  html:
    math:
      engine: mathjax
      path: path/to/local/mathjax.js
    code-tools: true
    code-fold: true
    code-summary: "Show code"
    html-table-processing: none
    df-print: kable
    code-block-border-left: "royalblue"
    code-block-bg: true
    embed-resources: true
    # monofont: sans
    self-contained: true
    number-sections: true
toc: true
lang: en
author:
  - name: Pietro Rota
    id: PR
    email: pietro.rota01@icatt.it
    affiliation: 
      - name: Università cattolica del sacro cuore
        city: Milano
        state: IT
        url: https://www.unicatt.it/
---

<head><link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" as="script"></head>

```{r setup, include = FALSE}
# library(quantmod)
# library(ggplot2)
# library(magrittr)
library(broom)
# library(dplyr)
# library(plotly)
library(hrbrthemes)
library("viridis")
# library(sde)
library(tseries)
library(Runuran)
library(moments)
library(fBasics)
library("stats4")
library(yuima)
library(VarianceGamma)
library(ghyp)
library(GeneralizedHyperbolic)
library(EnvStats)
library(Sim.DiffProc)
source("C:/Users/pietr/OneDrive/Desktop/formula.main.R")
knitr::opts_chunk$set(error=TRUE)
knitr::opts_chunk$set(warning = FALSE)
Sys.setlocale("LC_TIME", "C") # Set time locale to English
Sys.setlocale("LC_MESSAGES", "C") # Set messages to English
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(tidy=TRUE)


gghistogram <- function (x, add.normal = FALSE, add.kde = FALSE, add.rug = FALSE, bins, boundary = 0, 
                         fill = "#1f77b4", linewidth = 1, title = NULL, subtitle = NULL) {
    if (!requireNamespace("ggplot2", quietly = TRUE)) {
        stop("ggplot2 is needed for this function to work. Install it via install.packages(\"ggplot2\")", 
            call. = FALSE)
    }
    else {
        if (missing(bins)) {
            bins <- min(500, grDevices::nclass.FD(na.exclude(x)))
        }
        data <- data.frame(x = as.numeric(c(x)))
        binwidth <- (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/bins
        
        p <- ggplot2::ggplot() +
          ggplot2::geom_histogram(ggplot2::aes(x), data = data, binwidth = binwidth, 
                                  boundary = boundary, fill = fill) +
          ggplot2::labs(title = title, subtitle = subtitle)
          ggplot2::xlab(deparse(substitute(x)))
        if (add.normal || add.kde) {
            xmin <- min(x, na.rm = TRUE)
            xmax <- max(x, na.rm = TRUE)
            if (add.kde) {
                h <- stats::bw.SJ(x)
                xmin <- xmin - 3 * h
                xmax <- xmax + 3 * h
            }
            if (add.normal) {
                xmean <- mean(x, na.rm = TRUE)
                xsd <- sd(x, na.rm = TRUE)
                xmin <- min(xmin, xmean - 3 * xsd)
                xmax <- max(xmax, xmean + 3 * xsd)
            }
            xgrid <- seq(xmin, xmax, length.out = 512)
            if (add.normal) {
                df <- data.frame(x = xgrid, y = length(x) * binwidth * 
                  stats::dnorm(xgrid, xmean, xsd))
                p <- p + ggplot2::geom_line(ggplot2::aes(df$x, df$y), 
                                            col = "#ff8a62", linewidth = linewidth)
            }
            if (add.kde) {
                kde <- stats::density(x, bw = h, from = xgrid[1], 
                  to = xgrid[512], n = 512)
                p <- p + ggplot2::geom_line(ggplot2::aes(x = kde$x, y = length(x) * binwidth * kde$y), 
                                            col = "#67a9ff", linewidth = linewidth) 
            }
        }
        if (add.rug) {
            p <- p + ggplot2::geom_rug(ggplot2::aes(x))
        }
        return(p)
    }
}

bprint <- function(obj) {
  for (var in names(obj)) {
      cat(var, "=", as.numeric(obj[var]), "\n")
  }
}
```

```{r set x to time, include =FALSE}
# if (isrendering()) {
#   # Code that runs only during rendering
#   print(knitr::is_latex_output())
#   print(knitr::is_html_output())
#   library(purrr, quietly = T, warn.conflicts = F)
#   quickplot <- partial(quickplot, xlab = "time")
# }
```

<details>

<summary>Full dependencies list, click to expand list</summary>

```{r functions_loaded, echo=FALSE}
file <- "C:/Users/pietr/OneDrive/Documenti/aCATTOLICA ANNO 24-25/2 SEMESTRE/(ASF) - Applied Statistics for Finance/(ASF) - R/ASF/Applied statistics for finance.qmd"
when_rendering(functions_loaded(file))
```

Packages required to run this file

```{r required_packages, echo=FALSE}
when_rendering(required_packages(file))
```

Functions defined in this document

```{r required_functions, echo=FALSE}
when_rendering(required_functions(file))
```

</details>

# Random number generators and Monte Carlo

**Note on forward looking algorithms**

Much of this subject is based on a generative function that is forward looking of pseudo random values $x_{n+1} = f(x_n)$ given $x_{n+1}$ is not possible to obtain $x_n$ and given the numbers often times its not possible to invert the results and get back the function $f(x)$

## Random number generators

Computers follow deterministic patterns so there is no possibility of randomness, thats why we need to create it starting from the uniform distribution. It takes a number from 0 and 1 equally distributed set seed allows replicability, if you set the seed immediately before running `runif` you get the same numbers.

All sequences have the property that they are infinitely repeating the thing that changes is the seed, the sequence of numbers is from 0 to 1 and starts from a different point. the `set.seed` function allows to change the starting position of the seed, starting from the value in position `123` and stepping from there in the sequence

it's important to set the seed to allow for reproducibility of the work, when working with someone else or in different times

```{r Uniform distribution}
hist(runif(1000), nclass  = 1, xlim = c(0,3))
quickplot(runif(1000), title = "Uniform distributions", show_legend = F)

replicate(10, runif(20)) %>% quickplot(title = "Simulation of different Uniform distributions", linewidth = 0.6, show_legend = F)

set.seed(123)
runif(10)
set.seed(123)
runif(10)
```

for almost all applications of statistics a uniform distribution isn't enough, so you need a way to generate out of other distributions, there are two methods to do this, the inverese transformation and the acceptance rejection method.

For discrete random variables, the inverse $f^{-1}(x)$ is always easy to obtain like in the case of the exponential distribution (which is a continuous distribution but it works the same way). Instead of simulating from an exponential I can take the uniform distribution and isolate the $x$ as a function of $Unif(0,1)$, and from this inverse formula i can produce numbers out of it

$$F(x) = 1 − e^{−λx}$$

$$F(x)^{-1} \ \ \rightarrow  \ \ X= −\frac1λln(U)$$

Alternatively you could solve numerically and try and find parameters that get to the same results as the function, however it needs a lot of attempts to converge. So, it's generally not reccomended

```{r inverse}
# inverse transform method
X <- runif(1000)
lambda <- 5
Y <- exp(-lambda*X)
Y_inv <- (-1/lambda)*log(Y)
check_acc(X, Y_inv)
```

There are some cases that are more complex and you cannot invert it, even with the iterative process, for example the **normal distribution**, however you can solve it numerically, (still not reccomended)

This brings us to the so-called **acceptance-rejection method**.

Let $x$ be a random variable with density $f$ for which no RNG exists. Assume you know how to simulate from Y with density g and there exists a constant $c > 0$ such that

$$
\max \frac{f(x)}{cg(x)}< 1 \ \ \  \ \ \forall x
$$

Get the distribution $f(x)$ (target) and another distribution $g(x)$ (envelope) The 2 distribution need to be similar enough so that the distribution stays in the envelope but not so big that everything is rejected for this example lets use a normal PDF and a uniform distribution PDF

You can also work on $c$ which as it gets smaller there is less wasted area but also the risk of cutting off the area of the acutal function, so it needs to be somewhere in the middle, however if the distribution is a poor fit there is nothing that the $c$ can do on its own

```{r Acc Rej meth}
dn <- dnorm(-400:400/100)

c <- 1
data.frame(density = dn, x = (1:801) / 801, unif = 1) %>%
  ggplot(aes(x = x)) +
  geom_line(aes(y = density)) +
  geom_ribbon(aes(ymin = 0, ymax = 1), alpha = 0.3, color = "black") +
  xlim(c(0, 1.5)) +
  labs(title = "Acceptance-rejection method", subtitle = paste0("c=", c),
      x = paste0("Result of the function ", round(max(dn/(c*1)), 4)))


max(dn/(c*1))
```

In this case we can obviously see that the uniform distribution is not a good fit and we cant use it to approximate. Therefore i need a different distribution, the t-student seems to be a better fit, after looking for a better $c$ which is big enough so that $cg(x)$ can contain $f(x)$, but I want the bare minimum for that to happen: the bigger $cg(x)$ the higher the risk of falling into the rejection area and therefore of simulating values that I cannot use. but too small and the target spills from the envelope, meaning that those values will not be simulated and in these cases it's a big mistake to use this (graphically it's where the line is higher than the shaded area)

```{r Acc Rej 2}
c <- 2
dt_stud <- dt(x = -400:400 / 100, df = 1)
data.frame(density = dn, x = (1:801) / 801, tnorm = dt_stud) %>%
  ggplot(aes(x = x)) +
  geom_line(aes(y = density)) +
  geom_ribbon(aes(ymin = 0, ymax = tnorm * c), alpha = 0.3, color = "black") +
  labs(title = "Acceptance-rejection method", subtitle = paste0("c=", c),
       x = paste0("Result of the function ", round(max(dn/(c*dt_stud)), 4)))
max(dn/(c*dt_stud))


c <- seq(0.9, 3, length.out = 12)

aacplot <- function(c) {
  df <- data.frame(density = dn, x = (1:801) / 801, tnorm = dt_stud)

  ggplot(df, aes(x = x)) +
    geom_line(aes(y = density)) +
    geom_ribbon(aes(ymin = 0, ymax = tnorm * c), alpha = 0.3, color = "black") +
    geom_ribbon(aes(ymin = density, 
                    ymax = ifelse(density > tnorm, density, NA)), 
                fill = "red", alpha = 0.5) +
    ggtitle(label = NULL, subtitle = paste(paste0("c=", round(c, 2)), 
                                           "\nresult", round(max(dn / (c * dt_stud)), 4))) +
    theme_void()
}

suppressWarnings(grid.arrange(grobs = lapply(c, aacplot), nrow = 3))
```

in this very simple case a value of 1.66 is the best option

For any complex distribution some programmer went and developed the inverse to then apply from the uniform distribution

| **Random Variable** | **Generator**                              |
|---------------------|--------------------------------------------|
| Bin(n, p))          | `rbinom(r, n, p)`                          |
| Bin Neg(n, p)       | `rnbinom(r, n, p)`                         |
| Geom(p)             | `rgeom(r, p)`                              |
| Iperg(N,K, n)       | `rhyper(r,K, N − K, n)`                    |
| Poisson(λ)          | `rpois(r, λ)`                              |
| U(a, b)             | `runif(r, min = a, max = b)`               |
| Exp(a, b)           | `rexp(r,rate = λ)`                         |
| Normal(µ, σ)        | `rnorm(r, mean = µ,sd = σ)`                |
| $t_\nu$             | `rt(r, df = ν)`                            |
| $\chi_\nu^2$        | `rchisq(r, df = ν)`                        |
| F($ν1$, $ν2$)       | `rf(r, df1 = ν1, df1 = ν2)`                |
| Gamma(α, β)         | `rgamma(r,shape = α,rate = 1/β,scale = β)` |
| Beta(α, β)          | `rbeta(r,shape1 = α,shape2 = β)`           |

[Commmon question: When to use one or the other?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

You use the inverse transformation method when you can isolate $x$, from the function, typically with discrete distributions or the exponential distribution for continuous cases. If isolating $x$ is not possible, you use the acceptance-rejection method, being careful to use compatible distributions and working on $c$ to better the fit without cutting part of the distribution off.

## The Monte Carlo Method

The Monte Carlo Method is a numerical method based on statistical arguments which can be used to generate draws from a probability distribution and to evaluate integrals. In particular, the expected value of a random variable X with density $f(·)$ is an integral of this form

$$\mathbb E (X) = \int x·f(x)dx$$

we use the Monte Carlo Method to evaluate the expected payoff of some derivative in order to price it. This is what most of this course focuses on

thanks to the Law of Large Numbers in addition with the Central Limit Theorem we a correct estimation of the distribution ending points is the average

$$
\mathbb E\Big(g(Y)\Big) \cong \frac1n \sum_{i=1}^n g(y_i) = \bar g_n
$$

additionally we can draw the distribution based on this thanks to CLI and LLN

$$
\frac1n \sum_{i=1}^n g(y_i) \sim N\bigg(\mathbb E\Big(g(Y)\Big), \frac1nVAR\Big(g(Y)\Big)\bigg)
$$

and we can use this to actually price the option Get the mean and discount it to $t_0$ Stocks that don’t execute option still count and lower values $$Price=e^{-rt}⋅E[max⁡(0,S-K]$$

```{r Montecarlo}
MNC_Sim <- cbind(Sim.DiffProc::GBM(N = 100, M = 100, x0 = 50, theta = 0.05, sigma = 0.2), 
                 Sim.DiffProc::GBM(N = 100, M = 1, x0 = 50, theta = 0.50, sigma = 0.2)*2-50)
K <- 60
MNC_Sim %>% quickplot("Montecarlo simulation of geometric brownian motion", show_legend = F, x_size = 100)+
  geom_hline(aes(yintercept = K), linewidth = 1.2, color = "red")

mean_option <- mean(ifelse(last(MNC_Sim)>K, yes = last(MNC_Sim), 0))

cat("Mean of last datapoints", mean_option, "\n")
cat("Price of option according to MNC", exp(-1 * 1) * mean_option)
```

[Commmon question: Why use variance reduction techniques?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

In some cases, Monte Carlo intervals are not very informative if the variance of Y = g(X) is too large, as variability affects stability of the Monte Carlo method. This could be the case with some extreme outliers in the process (here done manually). In this case the process requires variance reduction techniques. They are discussed later when we can actually simulate GBM

The price of a derivative is given by the general formula

$$Pt = e^{−r (T−t)}\mathbb E{f(S_T)}$$

where $f(·)$ is some payoff function, e.g. $f (ST ) = max(S_T − K, 0)$ for a European call option with strike price K, maturity T. We need to estimate via Monte Carlo the expected value and, to this end, we need to be able to simulate the values of ST . So we need to know how to simulate stochastic processes.

$$S_t=x\cdot e^{\big(r-\frac{σ^2}{2}\big)\cdot t+σ\ W_t}$$ with $t>0$ and $x=S_0$

```{r lines}
replicate(15, cumsum(rnorm(100))) %>%
  quickplot(title = "Montecarlo simulation of rownian motion", subtitle = "No variance reduction", 
            show_legend = F, x_size = 100)
```

# Simulation of Stochastic Processes

A process that not only generates random numbers but also includes some drift component like the average growth coupled with a unexpected component.

A stochastic process is a collection of random variables indexed by time, typically denoted as ${X_t} t∈Γ$, where each $X_t$, maps outcomes from a probability space to real values, representing the evolution of a random system over time. It traces a trajectory or sample path, showing how the system behaves **over time under that specific model**. In practice, especially for simulation and numerical analysis, continuous-time processes are often approximated by evaluating them at finitely many discrete time points—called a time grid—which discretizes the interval into "grid-points", turning the process into a sequence of values at those specific times.

Trajectory or path of the random process, seen as a function of time. A trajectory represents the dynamic evolution of the stochastic process in time.

We divide this family of processes and variables along 2 axis and 4 categories

|          |           |                            |                           |
|:---------------:|:---------------:|:-----------------:|:-----------------:|
|          |           |      **State space**       |                           |
|          |           |          discrete          |         continous         |
| **Time** | discrete  | Random walks Markov Chains |       ARIMA, GARCH        |
|          | continous |          Poisson           | Lévy Wiener Diffusion SDE |

Continuous time and discrete time can be evaluated at any point in time vs a discrete model that only allows evaluation at only specified times

this is not possible in a practical and computational meaning, if you zoom infinitely you will always find space between the points.

So we include the concept of grid points (/time discretization points) allowing us to turn a continuous function to a discrete function $0→T$ divided into 100 points means that my “continuous time function” is discrete in 100 points

## Wiener process and basic Brownian motion

We use the notation $W_t$ or $B_t$ interchangeably

Brownian motion (or Wiener process) is a stochastic process $(B_t,t ≥ 0)$ starting from $0$ at $t_0$, i.e. $B0 = 0$, with the following properties:

**Independent increments** What happens from one discretization point to the next is all independent (yesterday doesn’t influence tomorrow)

$B_t − B_s$ and $B_u − B_v$ with $t,s,u,v$ being different points in time, independent for $(s,t) ∩ (v, u) = ∅$

**stationary increments** But the distribution assumption doesn’t change $dB = B_t-B_s$ depends only on $t − s$ and not by $t$ or $s$ separately

**Gaussian increments** most known property and most basic $dB \sim N(0,t − s)$ and$Bt \sim N(0,t)$

it requires first finding out your grid points, take your maturity time $T$ and divide it into as many steps you want $N$, `step_size = N` makes so that the x axis goes from 0 to $T$

$$W(t_i) = W(t_{i−1}) + z ·\sqrt{∆t}$$

[Very commmon question: Is the wiener process a markovian and a martingale?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br> these concepts will be introduced later...

Always markovian since you only take the last value, and a martingale since the average of the drift starting from a standard normal distribution is of mean 0

```{r wiener process}
N <- 100                 # number of end-points of the grid including Time
Time <- 1                # length of the interval [0, Time] in time units
Delta <- Time/N          # time increment (dt)
W <- numeric(N+1)        # initialization of the vector W
Time <- seq(0, 1, length=N)

for (i in 2:(N + 1)) {
  W[i] <- W[i - 1] + rnorm(1) * sqrt(Delta)
}

quickplot(W, title = "Simulation of Wiener processes", show_legend = F, x_size = N, linewidth = 1)
```

Different ways to make it more efficient using `system.time` to get the accurate time of the ways to compute, since `microbenchmark` gives a weird result sometimes, this means that from now on I will be using `BM.vec` to get the Brownian motion

```{r BM, warning=TRUE}
set.seed(123)
# brutal code
BM.1 <- function(N=10000){ 
  W <- NULL
  for(i in 2:(N+1))
         W <- c(W, rnorm(1) / sqrt(N))
  return(W)
}
system.time(BM.1())

set.seed(123)
# smarter
BM.2 <- function(N=10000){
  W <- numeric(N)
  Z <- rnorm(N-1)
  for(i in 2:(N))
         W[i] <- W[i-1] + Z[i-1] / sqrt(N)
  return(W)
}
system.time(BM.2())

set.seed(123)
# awesome!
BM.vec <- function(N = 10000) {
  W <- c(0, cumsum(rnorm(N-1) / sqrt(N-1)))
  return(W)
}
system.time(BM.vec())
```

Its no differentiable so you do it by hand basically by getting 2 points, one at $t=a$ and the next at $t=a+∆t$ and make a $∆t→0$ this gets you the value which goes to $+∞$

$$\lim_{∆t→0} \frac{|W (a + ∆t) − W (a)|}{∆t} ≃ \lim_{∆t→0} \frac{|\sqrt{∆t}|}{∆t}=+∞$$

```{r der BM}
dt <- seq(from = 0, to = 0.01, length.out = 100)
a <- 1
quickplot(abs((rnorm(1) * sqrt(a+dt)) - (rnorm(1) * sqrt(a)))/dt, 
          title = "Differential limit in 0 goes to infinity", linewidth = 1, show_legend = F)
```

Which as we said before we said that the lim is infinity, it means that the stochastic process you will not be using the component in of itself but you will use the integral (integral of the derivative cancel each other out and allows the process to not go to infinity)

A stochastic differential equation models the noise (or the stochastic part) of this system by adding the variation of some stochastic process to the above dynamics, e.g. the Wiener process

[Commmon question: What are the 2 components of a stochastic differntial equation?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

deterministic trend + stochastic noise

$$X_t=X_0+\int_{0}^{t}\Big(b(X_S)ds\Big)+\int_{0}^{t}\Big(\sigma\left(X_S\right)dW_S\Big)$$

## Geometric Brownian motion

**Why Is the Wiener Process Insuficicient?**

You can’t use the wiener process to simulate the stock path - It goes in the negative which stocks can’t - It starts from 0 - It has no trend So, we need a stochastic differential equation

A stochastic differential equation models the noise (or the stochastic part) of this system by adding the variation of some stochastic process to the above dynamics, e.g. the Wiener process, also it can go below $S_0$ but not below $0$

$$
\frac{X_{t+dt} − X_t}{dt} = \frac{dX_t}{dt}= b(X_t)
$$

$$
dX_t = b(X_t)dt + σ(X_t)dW_t
$$

Evolution through time $b(X_t)dt$ Deterministic trend $σ(X_t)$ Stochastic noise $dW_t$

Differential because it is based on increments derivatives Which as we said before we said that the lim is infinity, it means that the stochastic process you will not be using the component in of itself but you will use the integral (integral of the derivative cancel each other out and allows the process to not go to infinity)

[Commmon question: How is B&S modelled and what is the closed form solution?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

In the Black & Scholes theory of option pricing, we model an underlying asset using a stochastic process St called geometric Brownian motion which satisfies the stochastic differential equation

$$dS_t = µS_tdt + σS_tdW$$

Finally giving us the final closed formula solution

$$S_t=x_0\cdot EXP\bigg\{\bigg(r-\frac{\sigma^2}{2}\bigg)t+σW_t \bigg\} \ \ \ \ \ \  t>0 \ \ \ \ \ \ S_0=x$$

With all time steps all normally distributed, With no skew or kurtosis, so no spikes in the movement

No spike component means that it will trend to the drift component in the long run

```{r GBM.vec}
r <- 1
sigma <- 0.5
x <- 10
N <- 100                   # number of grid points including maturity Time
Time <- 1                  # length of the interval [0, Time] in time units
Delta <- Time/N            # time increment (dt)
W <- numeric(N+1)          # initialization of the vector W

Time <- seq(0, Time, length=N+1)

for(i in 2:(N + 1)) {
  W[i] <- W[i - 1] + rnorm(1) * sqrt(Delta)
}

S <- x * exp((r-sigma^2/2)*Time + sigma*W)

quickplot(data.frame(StockPath = S), 
          title = "geometric Brownian motion", x_size = N, linewidth = 1)


GBM.vec <- function(N = 100, x = 10, r = 1, sigma = 0.5, Time = 1) {
  # r = 1
  # sigma = 0.5
  # x = 10
  # N = 100                  # number of grid points including maturity Time
  # Time = 1                 # length of the interval [0, Time] in time units
  
  Delta <- Time/N            # time increment (dt)
  W <- numeric(N+1)          # initialization of the vector W
  
  Time <- seq(0, Time, length=N+1)
  for(i in 2:(N+1))
           W[i] <- W[i-1] + rnorm(1) * sqrt(Delta)
  
  S <- x * exp((r-sigma^2/2)*Time + sigma*W)
  return(S)
}

replicate(10, GBM.vec()) %>% 
  quickplot("Montecarlo simulation of geometric brownian motion", x_size = 100, show_legend = F)
```

## Variance reduction techniques

The most basic approach is to use antithetic sampling.

It is based on the assumption that if you take half of the distribution positive and the rest negative it leaves the the distribution unchanged (for example, if X is Gaussian, then −X is Gaussian as well).

Instead of simulating 1000 stock paths, I get only 500 from f(x) and the rest from f(-x) Which is the Antithetic part

Balancing back a outlier in the distribution with an equal number of simulations from the other side of the distribution, making it equally probable that I pick the same amounts of outliers

This only works when the distribution has the same distributional assumptions for $X$ and $-X$

**approach from slides**

$y_1$ is computed using x, while $y_2$ is computed using -x (its antithetic counterpart).

The function being evaluated resembles a payoff function (e.g., a put option with an exponential term).

The idea is that if $x$ produces an extreme value in one direction, $−x$ produces an opposite (but compensating) value, making the estimate more stable.

Compared to a theoretical benchmark an analytic formula for the expectation being estimated. Pricing formula for a European-style option

```{r VRT slides}
n <- 1000
beta <- 1
K <- 1
x <- rnorm(n)
y1 <- sapply(x , function(x) max(0 , K - exp(beta * x)))
y2 <- sapply(-x , function(x) max(0 , K - exp(beta * x)))
mean((y1 + y2) / 2) # MC with antithetic sampling
K * pnorm(log(K) / beta) - exp (beta^2 / 2) * pnorm(log(K) / beta - beta)
```

Graphical approach using geometric brownian motion, with the 2 equations being: A normal geometric brownian motion

$$S_t=x_0\cdot EXP\bigg\{\bigg(r-\frac{\sigma^2}{2}\bigg)t+σW_t \bigg\} \ \ \ \ \ \  t>0$$

And an inverse geometric brownian motion ($-\sigma W_t$)

$$S_t=x_0\cdot EXP\bigg\{\bigg(r-\frac{\sigma^2}{2}\bigg)t-σW_t \bigg\} \ \ \ \ \ \  t>0$$

```{r VRT graph}
# another function with  
GBM.vec_inv <- function(N = 100, x = 10, r = 1, sigma = 0.5, Time = 1) {
  # r = 1
  # sigma = 0.5
  # x = 10
  # N = 100                  # number of grid points including maturity Time
  # Time = 1                 # length of the interval [0, Time] in time units

  Delta <- Time / (N + 1)            # time increment (dt)
  W <- numeric(N)          # initialization of the vector W

  Time <- seq(0, Time, length = N)
  for (i in 2:N)
    W[i] <- W[i - 1] + rnorm(1) * sqrt(Delta)

  S <- x * exp((r - sigma^2 / 2) * Time - sigma * W)
  return(S)
}

N <- 100                # number of grid points including maturity Time
M <- 1000               # number of simulations (min 3)
x <- 10
r <- 1
sigma <- 0.5
Time <- 1

norm <- replicate(M,     GBM.vec(N = N, x = x, r = r, sigma = sigma, Time = Time))
inv  <- replicate(M, GBM.vec_inv(N = N, x = x, r = r, sigma = sigma, Time = Time))

long_norm <- melt(norm[,seq(from = 1, to = min(50,M), by =2)]) %>% 
  mutate(Var1 = Var1/N)
long_inv  <- melt( inv[,seq(from = 1, to = min(50,M), by =2)]) %>% 
    mutate(Var1 = Var1/N)

ggplot() +
  geom_line(data = long_norm, aes(x = Var1, y = value, group = Var2, colour = "Normal"), linewidth = 1) +
  geom_line(data = long_inv,  aes(x = Var1, y = value, group = Var2, colour = "Inverted"), linewidth = 1) +
  scale_color_manual(values = c(Normal = "darkblue", Inverted = "darkred")) +
  labs(title = ifelse(M > 50, paste("50 simulations out of", M, "total"), paste(M, "simulations")),
    subtitle = "Color coded split half normal half inverted", x = "time", y = "Value of GBM")

other_half_no_VRT <- replicate(M/2, GBM.vec(N = N, x = x, r = r, sigma = sigma, Time = Time))[N, ]
sim_no_VRT <- c(other_half_no_VRT, norm[N, ])

df <- data.frame(
  Category = c("BASE NORM", "BASE INV", "VRT", "NO VRT", "BASE OTHER HALF"),
  Variance = c(
    var(norm[N, ]),
    var(inv[N, ]),
    var(c(norm[N, ], inv[N, ])) ,
    var(sim_no_VRT),
    var(other_half_no_VRT)))

ggplot(df, aes(x = Category, y = Variance, fill = Category)) +
  geom_bar(stat = "identity") +
  labs(title = "Variance Comparison", x = "Category", y = "Variance") +
  theme(legend.position = "none") +
  coord_cartesian(ylim = c(min(df$Variance) - 10, max(df$Variance) + 10))  # Zoom without data loss
```

NORM variance: `r round(digits = 2, var(norm[N, ]))`

INV variance: `r round(digits = 2, var(inv[N,  ]))`

Variance with VRT of my simulations: `r round(digits = 2, var(c(norm[N, ],inv[N, ])))`

Variance no VRT, just the normal simulations and another M=`r M/2` simulations: `r round(digits = 2, var(sim_no_VRT))`

there is no guaranteed way to reduce variance this only happens when the the antithetic part actually reduces variance, it could have an extreme far bigger than the regular simulations and create more variance

Therefore, we have a variance reduction if $$Var \frac12 \bigg(g(X) + g\Big(h(X)\Big)\bigg) < Var\Big(g(X)\Big)$$

### Other examples of common stochastic processes in their closed formula

**Geometric Brownian motion (gBm)** $dX_t = µX_t d_t + σX_t dW_t$ Standard model for asset prices in Black-Scholes, lacks mean reversion.

**Cox-Ingersoll-Ross (CIR)** $dX_t = (θ_1 + θ_2X_t)dt + θ_3 \sqrt{X_t}dW_t$ Mean reverting property variation according to wiener process but mean reverting in $\lim(t)$ Ensures non-negative interest rates due to the square root term

**Chan-Karolyi-Longstaff-Sanders (CKLS)** $dX_t = (θ_1 + θ_2X_t)dt + θ_3X^{θ_4}_t dW_t$ mean reverting but with 2 variation component allowing more stray from trend Still no spikes component outright Generalizes CIR by allowing more flexible volatility behavior.

**Nonlinear mean reversion (Ait-Sahalia)** $dX_t = (α_{−1} X^{−1}_t + α_0 + α_1 X_t + α_2 X^2_t)dt + β_1X^ρ_t dW_t$ Can capture extreme deviations from equilibrium due to nonlinear drift.

**Double Well potential (bimodal behavior, highly nonlinear)** $dX_t = (X_t – X^3_t)dt + dW_t$ System fluctuates between two stable states, useful for modeling regime shifts.

**Jacobi diffusion (political polarization)** $dX_t = −θ\left(X_t - \frac12\right)dt +\sqrt{θX_t(1 – X_t)}dW_t$ Constrains values to the interval (0,1), suitable for proportions like voting shares.

**Ornstein-Uhlenbeck (OU)** $dX_t = θX_tdt + dW_t$ Also mean reverting also used for interest rates Widely used for modeling interest rates and volatility.

**Radial Ornstein-Uhlenbeck** $dX_t = (θX^{−1}_t - X_t)dt + dW_t$ Ensures positive values, often used in stochastic volatility models.

### SDE package

the sde package allows to simulate and use stochastic differential equations, the main function is `sde.sim` one of the main features of this function is the `model` which allows for multiple different complex models to be used `CIR` Cox-Ingersoll-Ross, `VAS` Vaisicek, `OU` Ornstein-Uhlenbeck, `BS` Geometric browninan motion used for Black and scholes

```{r SDE, message=FALSE}
n <- 100                        # number of simulation steps.
Delta <- 0.01                   #   time step of the simulation
S0 <- 10                        # fix initial value
mu <- 0.1                       # fix the drift value
sigma <- 0.2                    # and one for the volatility
 
## theta = vector of parameters for cdist
sde.sim(X0=S0, N=n, delta=Delta, model="BS", theta=c(mu, sigma)) %>% 
quickplot(title="Geometric Brownian motion", show_legend = F)


sde.sim(X0=S0, N=n, M = 10, delta=Delta, model="BS", theta=c(mu, sigma)) %>%
  quickplot("Montecarlo simulation of SDE Brownian motion", subtitle = "`M` allows to simulate more than one path", show_legend = F)
```

### Markov property

The Markov property is based on the filtration principle captures how much past conditional information a function has, as processes are backward-looking. For a 1-year filtration, each point has the past 252 trading days' data.

A stochastic process is Markovian if the conditional expectation of future values depends only on the current state, not the entire history: $$
E(X_n|F_{n-1}) = E(X_n|X_{n-1}, X_{n-2},\dots)= E(X_n|X_{n-1})
$$ A discrete-time process $\{X_n, n ≥ 1\}$ is Markovian if the conditional distribution of $X_n$ given the past equals its distribution given only $X_{n-1}$. Past values provide no additional information.

The Wiener process and geometric Brownian motion (gBm) are Markov processes—their closed-form solutions depend only on $x_0$. However, this lack of memory limits their ability to capture past patterns, as the gBm SDE doesn't use historical data after estimating parameters.

In short, for a Markov process, the entire history is equivalent to just the last data point.

### Martingale

A martingale is a stochastic process where the expected future value, given all past information, equals the current value. Formally, for a discrete-time process $\{X_n\}$:

$$
E(X_{n+1}|X_1,\dots,X_n)=X_n
$$

-   If $E(X_{n+1}|X_1, ..., X_n) ≥ X_n$ it is a submartingale. positive drift

-   If $E(X_{n+1}|X_1, ..., Xn) ≤ X_n$ it is a supermartingale. negative drift

Geometric Brownian Motion (gBm) is a martingale only when the drift parameter is zero. Otherwise:

This property reflects "fair" evolution (martingale), upward bias (submartingale), or downward bias (supermartingale).

Basically what is the drift, a true martingale is when the drift component is 0

[Commmon question: Is this a Martingale and Markovian process?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

In order for a sde with more than 2 components to be a martingale either they are all 0 **or** cancel each other out

| Model | Martingale? | Markov? |
|--------------------------|----------------------------|------------------|
| Wiener process (W) | Yes $E[N(0,1)]=0$) | ✅ Yes |
| Geometric Brownian Motion (gBm) | Only if drift = 0 (μ=0) | ✅ Yes |
| Cox-Ingersoll-Ross (CIR) | Can be $-θ_1=θ_2 \ \text{or} \ θ_1=θ_2=0$ | ✅ Yes |
| CKLS (Chan-Karolyi-Longstaff-Sanders) | No | ✅ Yes |
| Ornstein-Uhlenbeck (OU) | Can be if driftless | ✅ Yes |
| Variance Gamma (VG) | Can be if drift compensated $|b|=|a|$ | ✅ Yes |
| Meixner | Can be if symmetric | ✅ Yes |

### Discertization / Simulation strategies

they are all algorithms to estimate the values according to `sde`

3 types of discretization methods in `sde.sim` (From most narrow to most broad):

-   Exact sampling `EA` (GBM) if you know everything about the model and use either reverse or acceptance rejection method

-   Conditional distribution `cdist` (CIR OU GBM) Get the distribution starting from the starting value, since all are markovian once you have simulated all the values look at today, get the distribution conditional on the value of today

-   Discretization methods (Every other sde) take your function with the integral since there are no closed for solutions, solving both component numerically, least efficient so if you can use the others

-   Discretization methods: `euler` common, `milstein` another common, `KPS`, `milstein2`, `ozaki`, `shoji`. Discretization just means to have the integral of the sde and you need to solve it for every time step

$$ 
S_{t + dt} = S_t + \int_t^{t+dt} \mu (S_u,u)du + \int_t^{t+dt} \sigma (S_u,u)dWu
$$

Essentially this is 2 big integrals for mu and sigma

All of these can be used and to get the best result is to try many of them Empirically the only difference is that the Euler is just slower than the Milstein

The trend component in the process can be estimated with historical data and the estimate can be used to figure out the direction of the path

In levy processes there is another component with the jump so a third integral

[Commmon question: What is the difference between Millstein and Euler?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

2 discretization methods it just means that you are doing the integral of the sde and you need to solve it for every time step. All of these can be used and to get the best result is to try many of them. Empirically the only difference is that the Euler is just slower than the Milstein

```{r}
set.seed(1222)
Time <- 2
N <- 100
dt <- Time/N
X0 <- 40
mu_v <- 0.05
sigma_v <- 0.2
r <- 0.05
K <- 3


# Simulate GBM paths using different methods
gbm_euler <-    sde.sim(X0=X0, N=N, model="BS", theta=c(mu_v, sigma_v), method = "euler")
gbm_milstein <- sde.sim(X0=X0, N=N, model="BS", theta=c(mu_v, sigma_v), method = "milstein")
gbm_exact <-    sde.sim(X0=X0, N=N, model="BS", theta=c(mu_v, sigma_v), method = "EA")

# Plot paths
data.frame(euler = gbm_euler, milstein = gbm_milstein, exact = gbm_exact) %>% quickplot()

# dirty visualization on deliberately few paths to better see the convergence
nsim <- 20
euler_option <-  mean(pmax(0, last(replicate(nsim, sde.sim(X0 = X0, N = 1, T = Time, model = "BS", theta = c(mu_v, sigma_v), method = "euler"))) - K)) * exp(-r * Time)

milstein_option <- mean(pmax(0, last(replicate(nsim, sde.sim(X0 = X0, N = 1, T = Time, model = "BS", theta = c(mu_v, sigma_v), method = "milstein"))) - K)) * exp(-r * Time)

EA_option <- mean(pmax(0, last(replicate(nsim, sde.sim(X0 = X0, N = 1, T = Time, model = "BS", theta = c(mu_v, sigma_v), method = "EA"))) - K)) * exp(-r * Time)

BS_ref_option <- black_scholes(S = X0, K = K, Time = Time, r = r, sigma = sigma_v, type = "call")

data.frame(
  euler = c(euler_option, euler_option - BS_ref_option, abs(euler_option - BS_ref_option)),
  milstein = c(milstein_option, milstein_option - BS_ref_option, abs(milstein_option - BS_ref_option)),
  EA = c(EA_option, EA_option - BS_ref_option, abs(EA_option - BS_ref_option)),
  BS = c(BS_ref_option, NA, NA)
  ) %>% round(4) %>% set_rownames(c("Price", "Diff with BS", "Abs diff")) %>% 
  gt(rownames_to_stub = T) %>%
  data_color(direction = "row", rows = 3, na_color = "white", colors = c("green", "gainsboro", "red"), alpha = 0.2, autocolor_text = F) %>%
  tab_header("Price and difference with the BS function",subtitle = paste0("Deliberately few paths (", nsim, ") to better see the convergence"))
```

If your function is not one of the ones done in `sde` you can create 2 expressions one for the drift and one for the variance OU process, $dXt = −5X_tdt + 3.5dW_t$

The package uses by default the best method for the model if it is specified

```{r OU BY HAND}
d <- expression(-5 * x)
s <- expression(3.5)

X <- sde.sim(X0=10,drift =d,sigma =s)

quickplot(sde.sim(X0 = 10, drift = d, sigma = s, M = 100, ), title = "OU process user defined", show_legend = F)
```

CIR model $dXt = (6 − 3X_t)dt + 2\sqrt{X_t}dW_t$ via model name or, via exact conditional distribution rcCIR (also implemented in sde)

```{r CIR BY HAND}
d <- expression(6 - 3 * x)
s <- expression(2 * sqrt(x))
X <- sde.sim(X0 = 10, drift = d, sigma = s)
quickplot(X, title = "User-defined CIR")



set.seed(123)
X2 <- sde.sim(X0 =10, theta =c(6 ,3,2) , model = "CIR", rcdist =rcCIR, method ="cdist", M = 1000)
set.seed(123)
X_many <- sde.sim(X0 =10, drift = d, sigma = s, M = 1000)


X2_mean <- apply(X2, 1, mean)
X_many_mean <- apply(X_many, 1, mean)
check_acc(X2_mean, X_many_mean, title = "hand-made CIR and sde CIR across 1000 simulations")


grid.arrange(ncol=2, top = "100 simulations of both models", 
            X2[, 1:100] %>% 
              quickplot(title = 'Using model = "CIR"', show_legend = F),
            X_many[, 1:100] %>%  
              quickplot(title = "Defining expressions",show_legend = F))

```

# Time series analysis

`quantmod` is the industry standard of downloading financial data, the main provider is yahoo and uses the `xts` format which means that the data in actually indexed

```{r quantmod get}
Sq <- getSymbols("AAPL", from = "2015-02-17", to = "2025-02-17", auto.assign = F)

show_df(Sq, rounding = 2) %>% gt() %>% 
  sub_missing(columns = everything(), missing_text = "⋮") %>% 
  tab_header(title = "Apple dataset") %>%
  opt_stylize(style = 5, add_row_striping = TRUE)

paste("This data was downloadded from", attr(Sq, "src"))

quickplot(Cl(Sq), title = "AAPL close price")
```

from the `tseries` package it allows you to download data from many providers most importantly FRED(federal reserve databank) kind of like `quantmod` which is the industry standard though fImport is pretty much the same

```{r tseries get}
S <- get.hist.quote("AAPL", start = "2010-01-01", end = "2022-01-01")
chartSeries(S, TA=c(addVo(), addBBands()), theme="white")
S <- S$Close
```

## Log-returns

The time series is better suited to capture statistical properties in relative terms, rather than being affected by the size of the stock

Log returns normalize stock price movements, and allow for direct comparison between different stocks, regardless of their price level

Stock prices aren't statistically significant, so you need to use log returns, you can use a variety of ways to get them in this case we are using properties of logarithms, whrere the division is made by takint the subsequent difference of the logarithms of stock prices `na.omit` simply skips the days where there are no data recorded, may that be due to technical issues or whatever else

```{r logret}
X <- diff(log(S))
plot(X)
X <- na.omit(X)
```

## Change-point analysis

Volatility change-point estimator for diffusion processes based on least squares. in this case we can see that the price dynamics have changed during covid

Change point detection is used to check for biased data

[Commmon question: Why and how do you use cpoint?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

Change point detection (cpoint) in time series analysis is used to identify moments where the statistical properties of a series—such as the mean, variance, or correlation shift significantly.

The function highlights the exact moment when a structural break occurred. Data before the change point should not be used, as it does not reflect the stock’s current behavior.

This is especially important in fields like finance or economics, where events like financial crises, policy changes, or market shocks can introduce structural breaks that violate model assumptions like stationarity. Detecting these points allows analysts to segment the data for more accurate modeling, improve forecast reliability, and better understand the underlying dynamics or regime shifts in the data.

```{r sde cpoint}
cp <- cpoint(S)

bprint(cp)

addVLine = function(dtlist) plot(addTA(xts(rep(TRUE,NROW(dtlist)),dtlist),on=1,col="red"))
addVLine(cpoint(S)$tau0)

## Log-returns with change-point line
S <- as.numeric(S)
n <- length(S)
X <- log(S[-1]/S[-n])

plot(X, type = "l", main = "Apple stock log-returns")
abline(v = cpoint(X)$tau0, col = "red")
```

# Parameter estimation

-   Maximum likelihood estimation (MLE): uses the entire density function

-   Quasi-MLE (QMLE): uses a simplified version of the MLE

-   Method of moments: uses a few moments

## MLE maximum likelihood estimation

Objective: Estimate unknown parameter(s) $θ$ by maximizing the likelihood of observing the given data.

1.  Likelihood Function Discrete case:

$$ℓ(θ)=L_n(θ)=\prod_{i=1}^n p(x_i;θ)\quad (PMF)$$ $L_n (θ)$ is not a probability but a measure of how likely $θ$ is given the data. $L_n (θ)$ (likelihood of θ) from P(data) probability of data)

Log-Likelihood transforms products into sums for computational ease: Derivatives of sums are simpler than derivatives of products $$
LogL_n(\theta) = \sum_{i=1}^n log\ p (x_i;\theta)
$$ Log-returns $r_t = log(P_t/P_{t−1)}$ are often modeled as $N(μ,σ^2)$

```{r MLE}
cbind(dnorm(-1000:1000/100, mean = 4, sd = 3) ,
      dnorm(-1000:1000/100, mean = 2, sd = 3)) %>% quickplot() +
  geom_vline(aes(xintercept = 500))+
  labs(title = "MLE", subtitle = "vertical line is the unkonwn mean of my data, values increase as mu decreases")

cbind(dnorm(-400:400/100, mean = 1, sd = 1)) %>% quickplot(xlab = NULL) +
  geom_hline(aes(yintercept = 0.1))+
  geom_hline(aes(yintercept = 0.2))+
  geom_hline(aes(yintercept = 0.3))+
  geom_hline(aes(yintercept = 0.5), color = "red")+
  geom_hline(aes(yintercept = max(dnorm(-400:400/100, mean = 1, sd = 1))), color = "green")+
  labs(title = "Iterative process of trying to find the highest value",
       subtitle = "the likelihood keeps going until it overshoots and then comes back down")
```

lets say that the function is messier, you need limits optim methods and starting values in order to not fall into a local maximum and not the global maxima

```{r}
c(SMA(rnorm(801, sd = 20)/100, n = 50) + dnorm(-400:400/100, mean = 0, sd = 1)) %>% 
  quickplot(title = "Messy distribution with many obvious local maxima", x_start = -4, x_size = 101, show_legend = F)
```

Now lets start with a function that we know the distribution assumption of $N (5,2)$

using `stats4` package take the density function called whatever i want The mle function actually minimizes the negative log-likelihood $−ℓ(θ)$ as a function of the parameter $θ$ where $ℓ(θ) = log L(θ)$.

solve this procedure numerically (/iteratively) instead of starting values wherever you can already give it some starting values of the process, give also an upper value and a lower value to narrow the search down

[Commmon question: How to ensure that the value will converge?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

-   `function(mu = NUM, sigma = NUM)` these are the starting values, if you have an idea of where the parameters of the density is

-   `lower = c(0, 0) , upper = c(Inf, Inf)` set limits for the estimation, its not much use if the correct values are outside the boundaries but its way too much calculations if the boundaries are too large

-   `method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent")` choose the correct method of optimization

[Commmon question: What does the method do?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

The non linear optimization methods, picking different combinations of the parameter (in this case mu and sigma) to solve the likelyhood function methods are algorithms which tell R which combinations to use to calculate the log likelyhood, there isnt one best method, so depending on the shape of the distributional assumptions mle is based on `optim` which has 6 optimization functions in it

```{r MLE estimation}
# using the mle function
n <- 1000
xnorm <- rnorm(n, mean = 6, sd = 2)


log.lik <- function(mu = 2, sigma = 2) { # Choose good starting values
  -sum(dnorm(xnorm, mean = mu, sd = sigma, log = TRUE))
} 

fit <- mle(log.lik, lower = c(0, 0) , upper = c(Inf, Inf),  method = "L-BFGS-B")

summary(fit)

logLik(fit)

confint(fit)

check_acc(data1 = sort(xnorm), 
          data2 = sort(rnorm(n, mean = coef(fit)[1], coef(fit)[2])),
          title = "Sorted values to confront distributional assumptions")

data.frame(LR_sort = xnorm,
           norm_MLE = rnorm(n, mean = coef(fit)[1], coef(fit)[2])) %>%
  ggplot() +
  geom_histogram(aes(x = LR_sort, fill = "Norm"), alpha = 0.8, bins = 80) +
  geom_histogram(aes(x = norm_MLE, fill = "norm_MLE"), alpha = 0.8, bins = 80) +
  labs(title = "Distribution charts of both the real and the simulated data", x = NULL) +
  scale_fill_manual(name = NULL, values = c(Norm = "indianred3", norm_MLE = "olivedrab2")) +
  theme(legend.position = "bottom")
```

MLE fails bc the stock doesnt follow the GbM may be used with correlated regressors and data

```{r Using stock data}
# Delta <- 0.01         #FROM BEFORE          # time step of the simulation

x <- na.omit(diff(log(S)))
LR_S <- x

est.log.lik <- function(mu_v = 1, sigma_v = 1) {
  -sum(dnorm(x, 
             mean = (mu_v-0.5*sigma_v^2), 
             sd = sigma_v, 
             log = TRUE))
}

fit <- mle(est.log.lik, method = "L-BFGS-B", lower = c(0, 1e-6), upper = c(1, 2))

summary(fit)

logLik(fit)

confint(fit)

check_acc(data1 = sort(as.numeric(x)), 
          data2 = sort(rnorm(length(x), mean = coef(fit)[1], coef(fit)[2])), 
          title = "Sorted values to confront distributional assumptions")

  
  data.frame(LR_sort = as.numeric(x),
           MLE = rnorm(length(x), mean = coef(fit)[1], sd = coef(fit)[2])) %>%
  ggplot() +
  geom_histogram(aes(x = LR_sort, fill = "Log returns"), alpha = 0.8, bins = 80) +
  geom_histogram(aes(x = MLE, fill = "MLE"), alpha = 0.8, bins = 80) +
  labs(title = "Distribution charts of our real data and the one that we", x = NULL) +
  scale_fill_manual(name = NULL, values = c("Log returns" = "indianred3", MLE = "lightblue")) +
  theme(legend.position = "bottom")
```

```{r}
log_returns <- x
mu_prime <- mean(log_returns)
sigma_sq <- mean((log_returns - mu_prime)^2)  # MLE variance (divided by n)
sigma <- sqrt(sigma_sq)
mu <- mu_prime + 0.5 * sigma_sq  # MLE drift parameter

cat("Closed-form MLE Estimates:\n",
    "mu =", mu, "\n",
    "sigma =", sigma, "\n")
NLL <- function(mu, sigma) {
  mean <- mu - sigma^2 / 2  # Mean of log returns under GBM
  -sum(dnorm(log_returns, mean = mean, sd = sigma, log = TRUE))
}
# Initial parameters from closed-form estimates
start_params <- list(mu = 0.0001, sigma = 0.00001)

# Perform MLE with lower bound for sigma to ensure positivity
gbm_fit <- mle(minuslogl = NLL, start = start_params, 
               method = "L-BFGS-B", lower = c(-Inf, 1e-5))

# Display results
summary(gbm_fit)
mu_est <- coef(gbm_fit)["mu"]
sigma_est <- coef(gbm_fit)["sigma"]
mean_fit <- mu_est - sigma_est^2 / 2  # Fitted mean of log returns

hist(log_returns, breaks = 50, probability = TRUE,
     main = "Log Returns vs Fitted Normal Density",
     xlab = "Log Returns", col = "lightblue")
curve(dnorm(x, mean = mean_fit, sd = sigma_est), 
      col = "red", lwd = 2, add = TRUE)
legend("topright", legend = "Fitted Density", col = "red", lwd = 2)
qqnorm(log_returns, main = "Q-Q Plot of Log Returns")
qqline(log_returns, distribution = function(p) qnorm(p, mean = mean_fit, sd = sigma_est),
       col = "red", lwd = 2)

# Kolmogorov-Smirnov Test
ks.test(log_returns, "pnorm", mean = mean_fit, sd = sigma_est)
```

MLE Assumes we know the exact distribution of the data.

[Commmon question: Why are fat tails important?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br> Fat tails capture extreme events (crashes,bubbles) that the normal distribution misses. Ignoring fat tails leads to underestimating risk

## Quasi-MLE quasi-maximum likelihood estimation

From the package `yuima`

Allows for the possibility that the model might not be fully correct. It maximizes a **simplified approximation log-likelihood function** instead. Therefore it’s a lesser alternative of the MLE

Also can be used when the actual likelihood is complex or misspecified.

Even though QMLE may not be as efficient as MLE (because it might lose some information), it still gives consistent and asymptotically normal estimates, meaning that as the sample size grows, the estimates become reliable.

-   Similar to MLE but allows for possible model misspecification.

-   Uses a simpler version of the likelihood function (called a quasi-likelihood function).

-   Still gives consistent and asymptotically normal estimates, but they may be slightly less efficient than MLE.

set up from the yuima package $dX_t = −θ_2X_tdt + θ_1dW_t$ with $θ_1 = 0.3$ and $θ_2 = 0.1$.

```{r}
theta1 <- 0.3
theta2 <- 0.1

ymodel <- setModel (drift = c("(-1)*theta2*x"), diffusion = matrix(c("theta1"), 1, 1))
ymodel
n <- 100
ysamp <- setSampling(Terminal = (n) ^(1/3), n = n)
yuima <- setYuima(model = ymodel, sampling = ysamp)
set.seed(123)
yuima <- simulate(yuima, xinit = 1, true.parameter = list(theta1 = theta1, theta2 = theta2 ))
yuima


mle1 <- qmle(yuima, 
             start = list(theta1 = 0.8, theta2 = 0.7 ), 
             lower = list(theta1 =0.05, theta2 =0.05),
             upper = list (theta1 =0.5, theta2 =0.5), 
             method = "L-BFGS-B")

coef(mle1)
summary(mle1)
```

QMLE fails because the true model is not gBm, it looks somewhat alike but the fit is worse due to the simplification of the log likelihood function

```{r QMLE}
ymodel <- setModel(drift ="mu*x", diffusion = "sigma*x")
yuima <- setYuima(model = ymodel, data=setData(S), 
                  sampling=setSampling(delta=Delta,n=length(S)))

qmle <- qmle(yuima,
  start = list(mu = 0.05, sigma = 0.125),
  lower = list(mu = 0.025, sigma = 0.05), 
  upper = list(mu = 0.7, sigma = 0.5),
  method = "L-BFGS-B"
)

coef(qmle)

summary(qmle)

logLik(qmle)

check_acc(data1 = sort(as.numeric(LR_S)), 
          data2 = sort(rnorm(length(LR_S), mean = coef(qmle)[1], sd = coef(qmle)[2])), 
          title = "Sorted values to confront distributional assumptions")

data.frame(LR_sort = as.numeric(x),
           QMLE = rnorm(length(x), mean = coef(qmle)[1], coef(qmle)[2])
           ) %>%
  ggplot() +
  geom_histogram(aes(x = LR_sort, fill = "Log returns"), alpha = 0.8, bins = 80) +
  geom_histogram(aes(x = QMLE, fill = "QMLE"), alpha = 0.8, bins = 80) +
  labs(title = "Distribution charts of our real data and the one that we", x = NULL) +
  scale_fill_manual(name = NULL, values = c("Log returns" = "indianred3", QMLE = "palegreen")) +
  theme(legend.position = "bottom")
```

Using `cpoint` I was able to find the change during the covid pandemic and im using data from the change-point onwards Visual test of fitting analysis Density plot and QQ-plot the tails are different meaning that the returns dont follow a normal distribution function also AIC and we will see it later and a numerical value is a better option if you are not able to do parameter estiamtion using `MLE` you cant get the log-likelyhood and the AIC

$$AIC = −2l_n(\hatθ^{(ML)}_n)+ 2dim(\Theta)$$

its a tug of war btw the performance of the model and the number of parameter preventing overfitting by having just enough parameters and avoiding the unnecessary ones you can have a good fit by having a lot of parameters and

```{r Density plot and QQ-plot}
#| layout-ncol: 2

S <- Cl(getSymbols("AAPL", from = "2020-01-24", to = "2022-01-01", auto.assign = F))
X <- na.omit(diff(log(S)))

gghistogram(X, add.normal = T)
plot(density(X), lwd=2, main="Apple stock Density Plot")
f <- function(u) dnorm(u, mean=mean(X), sd=sd(X))
curve( f, -0.1, 0.1, add=TRUE, col="red",lwd=2)


gg_qq_plot(as.numeric(X), title = "APPLE Stock")

qqnorm(X, main = "Apple stock QQ plot")
qqline(X, col="red",lwd=2)
```

## Method of moments

1.  $µ$ is the real mean $E(X)$
2.  $σ$ is the real variance $Var(X)$
3.  $X$ is the sample mean
4.  $S^2$ is the sample variance

`mle` needs closed form, so if there is no closed form solution so for distributions that are too convoluted you need method of moments

The advantage that can always be calculated, however its not accurate,

When the parameters match with the 2 moments you can just calculate them basically as is, however in more complex distributions where this is not the case you need more complex processes Like with LEVY processes, with jump component, it has also a stochastic drift + jump component

```{mermaid}
flowchart LR
  A(MLE) --> B(QMLE) --> C(MoM)
```

Computing the sample moments

$$
  \begin{cases}
    E(X) = \frac\alpha\beta\\
    Var(X) = \frac\alpha{\beta^2}  
  \end{cases}
$$

$$
α = \frac{[E(X)]^2}{Var (X)} 
\ \ \ \text{and} \ \ \ 
β =\frac{E(X)}{Var(X)}
$$

```{r MoM}
# Mean, Variance:
x <- mean(as.numeric(LR_S))
y <- var(as.numeric(LR_S))
data.frame(EX = x, VarX = y)

# finding the Gamma distribution parameters via Method of Moments
alpha <- x^2 / y
beta <- x / y
data.frame(alpha, beta)

# Estimation of the historical mean and volatility of the GBM via Method of Moments
Delta_v <- 1 / 252
alpha.hat <- mean(LR_S, na.rm = TRUE) / Delta_v
sigma.hat <- sqrt(var(LR_S, na.rm = TRUE) / Delta_v)
mu.hat <- alpha.hat + 0.5 * sigma.hat^2

data.frame(
  sigma.hat = as.numeric(sigma.hat),
  mu.hat = as.numeric(mu.hat)
)

check_acc(data1 = sort(as.numeric(LR_S)), 
          data2 = sort(rnorm(length(LR_S), mean = mu.hat, sd = sigma.hat)), 
          title = "Sorted values to confront distributional assumptions")

data.frame(LR_sort = sort(as.numeric(LR_S)),
           MLE = sort(rnorm(length(LR_S), mean = mu.hat*Delta_v, sigma.hat*Delta_v))) %>%
  ggplot() +
  geom_histogram(aes(x = LR_sort, fill = "Log returns"), alpha = 0.8, bins = 80) +
  geom_histogram(aes(x = MLE, fill = "MOM"), alpha = 0.8, bins = 80) +
  labs(title = "Distribution charts of our real data and MoM", x = NULL) +
  scale_fill_manual(name = NULL, values = c("Log returns" = "indianred3", MOM = "#fea966")) +
  theme(legend.position = "bottom")
```

[Commmon question: Advantage and dissadvantages of MoM?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

With respect The advantage with method of moments is that if you don’t have the closed form density function you still have the moments of the density if not the whole distribution too convoluted or no closed form You are not using the whole distribution and so the estimate of the parameter will not be as accurate as the MLE which uses the whole distribution

No density needed—works with only moment conditions.

Mle is better because it takes all the distribution MoM is only to be used when density function is not known in closed form or doesn’t exist and therefore not as robust QMLE is simplifies the MLE process and therefore it’s a lesser alternative of the MLE

[Commmon question: What is AIC and when do we use it?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

The aim is to try to identify the underlying continuous model on the basis of discrete observations using AIC (Akaike Information Criterion) statistics defined by Akaike (1973, 1974) $$
AIC = −2 * ln\Big(\hatθ^{ML}_n\Big)+ 2\text {dim}(Θ)
$$ - $ln\Big(\hatθ^{ML}_n\Big)$: Log likelihood from fitting process

-   $2\text {dim}(Θ)$: 2 \* number of parameters used

When comparing several models for a given dataset, the model such that the AIC is lower is preferred.

[Commmon question: Why does AIC only work in MLE is used?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br> Because AIC depends directly on the log-likelihood, and this is only maximized in the context of MLE. If you estimate parameters with Method of Moments (MoM), there’s no valid likelihood to compute.

# European options

## Scheme to option pricing

1.  Download the time series of the underlying asset, polish it up from NAs and from biased data (Change point detection)
2.  Choose a distributional assumption/stochastic process which you think would fit well on your data
3.  Do parameter estimation for that chosen distributional assumption
4.  Do some tests of fitting: if the chosen distributional assumption fits well move to step 5, if it doesn’t go back to Step 2 and try some other distributional assumption, estimate the parameters and do tests of fitting till you find a distributional assumption that fits well on your data;
5.  Run your simulations and risk-neutralize before or after running your simulations depending on the risk-neutralization method
6.  Apply your boundary conditions and price the option.

## Risk neutralization

Starting from the concept of risk neutrality with all instruments and all values being priced in the same way in order to not have an advantage based on the positions taken using $Q$ like in $E_Q$ to say that you are in the risk neutral risk space

starting from the risk neutral call price you can get back the risk neutral stock price path calculation

like with the martingale property of markovian processes $E(X_{n+1}|X_1, ..., X_n) = X_n \rightarrow S_0 = e^{-rT}E_Q[S_T]$ with the discounted process being a martingale process

**This theorem where the discounted price process needs to be amartingale is called the Girsanov theorem** (named after Igor Vladimirovich Girsanov)

From B&S we seen that it starts from the SDE (slide 20 stoch processes) to a closed solution in the process you get rid of $\mu$ to $r$ as part of the risk neutralization procedures

$$S_0 = e^{-rT}E_Q[S_T] = S_0 = e^{-rt} *S_0 EXP\{ \mu d_T +\sigma dW_T \}$$ This sde is not a martingale bc there is a drift component if $\mu \neq 0$

To risk-neutralise a GbM we can just keep $σ$, but substitute the parameter $µ$ with the risk-free rate $r$, therefore the risk-neutral parameters are $(r, σ)$

substitute $\mu$ with the riskfree rate $r$

Meaning that black and scholes follows the Girsanov property

[Commmon question: Why risk neutralize?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

To ensure arbitrage-free pricing of financial derivatives, we must work in a risk-neutral framework—where prices are derived without dependence on individual risk preferences.

No-Arbitrage Condition: In efficient markets, arbitrage opportunities cannot persist. Prices must adjust to eliminate them.

### Risk neutralization methods

2 main methods Escher transformation "a priori" risk neutralization and mean-correcting-martingale is "a posteriori", easier to use. You can simulate and then riskneutralize at the end

There isnt a best one, you need to try them and check them witht the market prices

Some of the easiest are the mean correcting martingale and the Esscher transformation

**Esscher trasnformation**

A priori risk neutralization method of getting the phisical parameteres and insert them in a specific formula

$$f_θ(x) =\frac{exp(θx)f(x)}{\int_R exp(θx)f(x)dx}$$

f(x) is our assumption then the expectations of x multyplied by $\theta$ trying a bunch of potential theta to make sure that the girsanov theorem is fufilled

**Mean correcting martingale**

a posteriori risk neutralization method estimate the parameters and simulate the matrix of simulations using the phisical values, then put the matrix into the funky formula

$$S^{RN}_{M,M}= S0 \ exp(SM,N) \frac{exp(rt)}{E[exp(S_{,N+1})]}$$

matrix of simulation with `sde.sim` at the numerator times(/divided) S_0 and $e^{rt}$ divided by the same matrix's expectations for every time point (/line in the matrix) by using the mean, creating a vector of N+1 elements

```{r IDK_MCM}
# Parameters
S0 <- 100      # Initial stock price
mu <- 0.05     # Drift
sigma <- 0.2   # Volatility
Time <- 1         # Time horizon (years)
n <- 252       # Number of time steps
M <- 100      # Number of simulated paths
r <- 0.03      # Risk-free rate

# Simulate paths using sde.sim
S_paths <- sde.sim(X0 = S0, model = "BS", theta =c(mu, sigma), T = Time, N = n, M = M)

# Mean of exponentiated paths at each time step
E_S_N1 <- apply(S_paths, 1, mean) 

# Apply Mean-Correcting Martingale (MCM) formula
S_RN_MCM <- S0 * S_paths * exp(r * (1:N) * (Time/N)) / E_S_N1

grid.arrange(top = paste("Simulated Stock Price Paths before and after MCM\n Showing", min(30, M), "simulations"), ncol =2, 
quickplot(S_paths[,1:min(30, M)], show_legend = F, title = "BEFORE"),
quickplot(S_RN_MCM[,1:min(30, M)], show_legend = F, title = " AFTER", ylab = NULL))


# Plot the original GBM paths vs. the Mean-Correcting Martingale
ggplot() +
  geom_line(data = melt(S_paths[,1:min(30, M)]), aes(x = Var1, y = value, color = "GBM Paths", group = Var2),
            linewidth = 1) +
  geom_line(data = melt(S_RN_MCM[,1:min(30, M)]), aes(x = Var1, y = value, color = "MCM Paths", group = Var2),
            linewidth = 1) +
  labs(title = "Stock Price Paths vs. Mean-Correcting Martingale Adjusted Paths", 
       subtitle = paste("Showing", min(30, M), "simulations"),
       x = "Time (Days)", y = "Stock Price") +
  scale_color_manual(values = c("#131cce", "red")) +
  theme(legend.title = element_blank(), legend.position = "bottom")


grid.arrange(top = "Boxplot to compare how the distribution changes across time steps", ncol=2,
melt(data.frame(t(as.data.frame(S_paths)))) %>%
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot(fill = "#131cce", color = "navyblue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Before", x = NULL, y = NULL)+
    theme(
    axis.text.x = element_blank(),
    panel.grid.minor.x = element_blank(),  # Remove minor x-axis grid lines
    panel.grid.major.x = element_blank())  # Keep major x-axis grid lines
,

melt(data.frame(t(as.data.frame(S_RN_MCM)))) %>%
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot(fill = "red", color = "darkred") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "After", x = NULL, y = NULL) +
  theme(
    axis.text.x = element_blank(),
    panel.grid.minor.x = element_blank(),  # Remove minor x-axis grid lines
    panel.grid.major.x = element_blank()  # Keep major x-axis grid lines
  )
)
```

```{r deepseek attempt at MCM, eval=FALSE}
# Set seed for reproducibility
set.seed(42)

# Parameters
S0 <- 100       # Initial stock price
r <- 0.05       # Risk-free rate
T <- 1          # Total time (1 year)
N_steps <- 20  # Number of time steps (daily)
N_paths <- 10 # Number of simulated paths

# Simulate log-prices (Brownian motion)
log_S_paths <- matrix(rnorm(N_paths * N_steps, mean = 0, sd = 0.01), nrow = N_paths, ncol = N_steps)
log_S_paths <- t(apply(log_S_paths, 1, cumsum))  # Cumulative sum (log-price)

# Convert to actual prices
S_paths <- S0 * exp(log_S_paths)

# Time grid
dt <- T / N_steps
t_grid <- seq(dt, T, by = dt)

# Martingale correction function
mean_correcting_martingale <- function(S_paths, S0, r, t_grid) {
  N_paths <- nrow(S_paths)
  N_steps <- ncol(S_paths)
  S_RN <- matrix(0, nrow = N_paths, ncol = N_steps)
  
  for (t in 1:N_steps) {
    # Expected value of S_{t+1} (if t < N_steps)
    if (t < N_steps) {
      E_S <- mean(S_paths[, t + 1])
    } else {
      # If at last step, approximate E[S_{t+1}] as S_t * exp(r * dt)
      E_S <- mean(S_paths[, t]) * exp(r * dt)
    }
    
    # Apply martingale correction
    S_RN[, t] <- S0 * S_paths[, t] * exp(r * t_grid[t]) / E_S
  }
  
  return(S_RN)
}

# Apply correction
S_RN <- mean_correcting_martingale(S_paths, S0, r, t_grid)

# Verify martingale property: E[S_RN[, t]] should ≈ S0 * exp(r * t)
# for (t in 1:N_steps) {
#   cat(
#     "t =", round(t_grid[t], 3), 
#     "| E[S_RN] =", round(mean(S_RN[, t]), 2), 
#     "| S0 * exp(r*t) =", round(S0 * exp(r * t_grid[t]), 2), 
#     "\n"
#   )
# }
```

## B&S pricing

No need for boundary conditions in the case of a plain vanilla option

```{r BS pricing}
# CALL formula
call.price <- function(x = 1, Time = 0, T_mat = 1, r = 1, sigma = 1, K = 1) {
  d2 <- (log(x / K) + (r - 0.5 * sigma^2) * (T_mat - Time)) / (sigma * sqrt(T_mat - Time))
  d1 <- d2 + sigma * sqrt(T_mat - Time)
  price <- x * pnorm(d1) - K * exp(-r * (T_mat - Time)) * pnorm(d2)
  return(price)
}

# PUT formula
put.price <- function(x = 1, Time = 0, T_mat = 1, r = 1, sigma = 1, K = 1) {
  d2 <- (log(x / K) + (r - 0.5 * sigma^2) * (T_mat - Time)) / (sigma * sqrt(T_mat - Time))
  d1 <- d2 + sigma * sqrt(T_mat - Time)
  price <- K * exp(-r * (T_mat - Time)) * pnorm(-d2) - x * pnorm(-d1)
  return(price)
}

# Example
S0 <- 100
K <- 110
r <- 0.05
Time <- 1/4
sigma <- 0.25
C <- call.price(x = S0, Time = 0, T_mat = Time, r = r, K = K, sigma = sigma)
cat("Call price according to Black and Scholes", C, "\n")

P <- put.price(x = S0, Time = 0, T_mat = Time, r = r, K = K, sigma = sigma)
cat("Put price according to Black and Scholes", P, "\n")


# Put-Call parity
P2 <- C - S0 + K * exp(-r * Time)
cat("Put price according to Put call parity", P2, "\n")
```

## Montecarlo Pricing

Your MCPrice function is implementing Monte Carlo pricing for European-style options using antithetic variates to reduce variance. Specifically, it prices a European call or put option based on the payoff function `f(x)`, where `x` represents the final stock price at maturity.

**Stock Price Simulation:** It simulates terminal stock prices under the Black-Scholes model $$S_T=S_0 \cdot EXP\left\{(r− \frac12 σ^2)(T−t)+σ\sqrt{T−t}Z\right\} $$ It also generates antithetic variates (i.e., both $u$ and $-u$), which improves efficiency by reducing variance

**Payoff Computation** The fun `f(xx)` represents the option payoff, meaning you can define different options: For a European call option: `f <- fun(x) max(0, x - K)` For a European put option: `f <- fun(x) max(0, K - x)`

**Discounting the Expected Payoff:** The expected payoff is discounted using the risk-free rate $e^{−r(T−t)}$

here we are also doing antithetical sampling as a form of variance reduction. taking half of the simulations `m` from from `rnorm(m / 2)` and the other half from `- rnorm(m / 2)`

```{r Monte Carlo pricing}
MCPrice <- function(x = 1, Time = 0, T_mat = 1, r = 1, sigma = 1, M = 1000, f) {
  h <- function(m) {
    u <- rnorm(m / 2)
    tmp <- c(
      x * exp((r - 0.5 * sigma^2) * (T_mat - Time) + sigma * sqrt(T_mat - Time) * u),
      x * exp((r - 0.5 * sigma^2) * (T_mat - Time) + sigma * sqrt(T_mat - Time) * (-u))
    )

    mean(sapply(tmp, function(xx) f(xx)))
  }
  p <- h(M)
  p * exp(-r * (T_mat - Time))
}
# Example
f <- function(x) max(0, x - K)


M <- 1000
MC1 <- MCPrice(x = S0, Time = 0, T_mat = Time, r = r, sigma, M = M, f = f)
cat("Call price according to Monte Carlo", MC1, "after", M, "simulations, with a diffference of", MC1-C, "\n")


M <- 50000
MC2 <- MCPrice(x = S0, Time = 0, T_mat = Time, r = r, sigma, M = M, f = f)
cat("Call price according to Monte Carlo", MC2, "after", M, "simulations, with a diffference of", MC2-C, "\n")


M <- 1e+06
MC3 <- MCPrice(x = S0, Time = 0, T_mat = Time, r = r, sigma, M = M, f = f)
cat("Call price according to Monte Carlo", MC3, "after", M, "simulations, with a diffference of", MC3-C, "\n")

# Speed of convergence
m <- c(10, 50, 100, 150, 200, 250, 500, 1000)
p1 <- NULL
err <- NULL
nM <- length(m)
repl <- 100
mat <- matrix(, repl, nM)
for (k in 1:nM) {
  tmp <- numeric(repl)
  for (i in 1:repl) {
    tmp[i] <- MCPrice(x = S0, Time = 0, T_mat = Time, r = r, sigma, M = m[k], f = f)
  }
  mat[, k] <- tmp
  p1 <- c(p1, mean(tmp))
  err <- c(err, sd(tmp))
}

colnames(mat) <- m
mat %>% round(5) %>%  datatable()
```

With the law of large numbers you can see that the values converge to the black and scholes value

```{r show mc}
p0 <- C
minP <- min(p1 - err)
maxP <- max(p1 + err)
plot(m, p1, type = "n", ylim = c(minP, maxP), axes = F, ylab = "MC price",  xlab = "MC replications")
lines(m, p1 + err, col = "blue")
lines(m, p1 - err, col = "blue")
axis(2, p0, "B&S price")
axis(1, m)
boxplot(mat, add = TRUE, at = m, boxwex = 15, col = "orange",  axes = F)
points(m, p1, col = "blue", lwd = 3, lty = 3)
abline(h = p0, lty = 2, col = "red", lwd = 3)


# Assuming mat is a matrix: rows = replications, columns = sample sizes
# Convert matrix to long format
mat_long <- as.data.frame(mat)
mat_long$rep <- 1:nrow(mat_long)
mat_long <- pivot_longer(mat_long, cols = -rep, names_to = "m", values_to = "MC_price")
mat_long$m <- as.numeric(gsub("V", "", mat_long$m))  # Convert m to numeric if needed

# Create a summary frame for lines and CI bands
summary_df <- data.frame(
  m = m,
  p1 = p1,
  upper = p1 + err,
  lower = p1 - err
)

summary_df %>% round(4) %>% datatable()

# Plot
ggplot(mat_long, aes(x = factor(m), y = MC_price)) +
  geom_boxplot(fill = "orange", outlier.shape = 1, width = 0.5) +
  geom_point(data = summary_df, aes(x = factor(m), y = p1), color = "blue", size = 2) +
  geom_line(data = summary_df, aes(x = factor(m), y = upper, group = 1), color = "blue") +
  geom_line(data = summary_df, aes(x = factor(m), y = lower, group = 1), color = "blue") +
  geom_hline(yintercept = C, linetype = "dashed", color = "red", size = 1) +
  labs(x = "MC replications", y = "MC price")
```

## FFT pricing

Moment generating function with a complex number $i=\sqrt{-1}$ From the density function the FFT adjusts from the real domain to the immaginary domain and then using the inverse transform of this function and get the oppposite value to the real domain $φ(t) = E\{e^{itX} \}$

Its a form of pricing to be used as a last resort, when possible you need to be using montecarlo simulations

[VERY Commmon question: What is Fast Fourier Transformation? when to use it? how to improve convergence?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

The Fast Fourier Transform (FFT) method is especially useful in option pricing. It relies on the Characteristic function(a transformation of the random variable into the imaginary domain), which is the imaginary domain, so there is a problem where the number is not a real number like with pricing, but you can take the inverse of this function and get it back to the real domain giving you the end price

It can be used if the density function doesn’t exist or is too convoluted and cannot be determined in closed-form Density function is real domain, or is too mathematically complex to work with directly—as is often the case with multi-asset models or models with jumps

A key advantage is that this function always exists, even when the density function does not. Since characteristic functions live in the complex (imaginary) domain, we use an inverse Fourier transform to retrieve results in the real domain, such as option prices. It also have some properties that allows for precise pricing in many advanced models (multi-asset models)

To enhance the numerical stability and convergence speed of this inversion, a Dampening is an "hyper-factor" (usually denoted $\mu$) is introduced. This dampening acts as a smoothing parameter (for ensuring that the integral in the inverse transform is well-behaved). A typical value for $\mu$ in the geometric Brownian motion setting is 1, but it can be adjusted to improve convergence when using FFT.

```{r FFT method}
FFTcall.price <- function(phi, S0, K, r, Time, alpha = 1, N = 2^12, eta = 0.25) {
  m <- r - log(phi(-(0 + 1i)))
  phi.tilde <- function(u) (phi(u) * exp((0 + 1i) * u * m))^Time
  psi <- function(v) {
    exp(-r * Time) * phi.tilde((v - (alpha + 1) * (0 + 1i))) /
      (alpha^2 + alpha - v^2 + (0 + 1i) * (2 * alpha + 1) * v)
  }
  
  lambda <- (2 * pi) / (N * eta)
  b <- 1 / 2 * N * lambda
  ku <- -b + lambda * (0:(N - 1))
  v <- eta * (0:(N - 1))
  tmp <- exp((0 + 1i) * b * v) * psi(v) * eta * (3 + (-1)^(1:N) - ((1:N) - 1 == 0)) / 3
  ft <- fft(tmp)
  res <- exp(-alpha * ku) * ft / pi
  inter <- spline(ku, Re(res), xout = log(K / S0))
  return(inter$y * S0)
}

phiBS <- function(u) {
  exp((0 + 1i) * u * (mu - 0.5 * sigma^2) - 0.5 * sigma^2 * u^2)
}

mu <- 1

FFT <- FFTcall.price(phiBS, S0 = S0, K = K, r = r, Time = Time)

cat("Call price according to Monte Carlo", FFT, "with a diffference of", FFT-C, "\n")
```

# Levy processes

Starting from the limitations of the GbM Paul Levy devised a new kind of stochastic process where spikes are possible

1.  Distributions are not normal
2.  correlations converge on one
3.  markets are not efficient
4.  investors and traders are deeply flawed
5.  there is no such thing as a rational investor
6.  there are many anomalies

Levy processes were introduced as the sum of a jump process and a Brownian motion with drift. The original idea was to construct a family of processes wide enough to comprise a variety of well-known other stochastic processes.

with the summation of 2 different processes $X_t$ and $M_t$

$$Z_t = X_t + M_t$$

The initial process based on a GbM

$$
X_t = µt + σB_t   \ \ \ \ \ \ µ ∈ \mathbb R, \ \ σ ≥ 0
$$

And the second

$$M_t =\sum ^{N_t}_{i=0}Y_{τi} − λ_t \mathbb E(Y_{τi}) \ \ \ λ ≥ 0$$

with $N_t$ an homogenous Poisson process which counts the random number of jumps and $Y_{τi}$ a sequence of i.i.d. random variables which represent the entity of the jumps

This process, within two random jumps, has a continuous paths.

A stochastic process ${Zt , 0 ≤ t ≤ T}$, with $Z0 = 0$ almost surely is called a Levy process if the following properties are satisfied

-   $Z_t$ has independent increments, i.e. $Zt − Zs$ is independent of $I_s$ , for any $0 ≤ s < t ≤ T$

-   $Z_t$ has stationary increments, i.e. for any $0 ≤ s, t ≤ T$ the distribution of $Zt+s − Zt$ does not depend on $t$

-   $Z_t$ is stochastically continuous, i.e. for every $0 ≤ t ≤ T$ and $ϵ> 0$ we have that $\lim_{s→t} P(|Zt − Zs | > ϵ) = 0$

**Levy-Khintchine Formula** Lévy process is characterized by its characteristic function, which is given by the Lévy–Khintchine formula

$$
\mathbb E [e^{iuX}] = exp\left[ibu − \frac{u^2c}2 + \int _\mathbb R \left(e^{iux} − 1 − iux \boldsymbol 1_{|x|<1}\right) ν(dx)\right]
$$

**Levy triplets** In the above, $\mathbf {1}$ is the indicator function. Because characteristic functions uniquely determine their underlying probability distributions, each Lévy process is uniquely determined by the "Lévy–Khintchine triplet" $(b,\ c,\ ν)$ or on wikipedia $(a,\sigma ^{2},\Pi)$. The terms of this triplet suggest that a Lévy process can be seen as having three independent components: a linear drift, a Brownian motion, and a Lévy jump process

Any process that has this levy triplets (AKA triplets of levy char) can be a levy process The law PX of a random variable X is infinitely divisible if and only if there exists a triplet $(b,\ c,\ ν)$, such that:

1.  drift term $b \in \mathbb R$

2.  diffusion coefficient $c \in \mathbb R \geq 0$

3.  Levy measure ν satisfying the property $\int _\mathbb R (1 ∧ |x|^2) ν(dx) < \infty$

GbM can be a levy process with levy measure = 0 any sde can be a levy

[Commmon question: Are levy processes martingale and markovian?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br> Submartingale more than supermartingale gamma distribution tails. And a markovian process, still only the last point filtration

```{r levy process data}
startLV <- Qdate(9, 10, 2017)
ticker <- "TSLA"
S <- Cl(getSymbols(ticker, from = startLV, to = "2024-02-16", auto.assign = F))
cp <- cpoint(as.numeric(S))

bprint(cp)

ggplot(S, aes(x = index(S), y = S))+
  geom_line()+
  geom_vline(aes(xintercept = startLV+cp$tau0), color ="red")+
  labs(title = paste("Price of", ticker, "with changepoint"), x = "Date", y = "Price")

S <- Cl(getSymbols(ticker, from = startLV + cp$tau0, to = "2024-02-16", auto.assign = F))
l_ret <- na.omit(diff(log(S)))
```

Following the scheme for option pricing, i need to take the prices from the changepoint onwards, instead of `r startLV`, i'm taking the data from `r startLV + cp$tau0`

VARIANCE-GAMMA MODEL: using the `VarianceGamma` package to get the prameters [Commmon question: What are the different methods?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br>

1.  "BFGS" uses the quasi-Newton method "BFGS" as documented in optim;

2.  "Nelder-Mead" uses an implementation of the Nelder and Mead method as documented in optim, most of the time this is that reaches convergence faster

3.  "nlm" Newton Raphson method uses the nlm function in R.

choosing based on the Log likelyhood, **however there is evidence that the Nelder-Mead is the best to achieving convergence**

```{r levy process}
# Parameters fitting
vgfit <- vgFit(l_ret, ) # esitmate VG parameters on the sample
summary(vgfit)

bprint(vgfit$param)

## Assign parameters
vg_param <- as.numeric(vgfit$param)
c <- vg_param[1]
sigma <- vg_param[2]
theta <- vg_param[3]
nu <- vg_param[4]

Time <- 1/4  # option maturity = 3 months
N <- 100     # number of steps for each path
r <- 0.01    # arbitrary risk-free rate
nsim <- 100  # number of simulated path

# Variance Gamma function
# Variance Gamma function
VG <- function(sigma, nu, theta, Time, N, r) {
    a <- 1/nu
    b <- 1/nu
    h <- Time/N
    Time <- (0:N) * Time/N
    X <- rep(0, N + 1)
    I <- rep(0, N)
    X[1] <- 0

    for (i in 1:N) {
        I[i] <- rgamma(1, a * h, b)
        X[i + 1] <- X[i] + theta * I[i] + sigma * sqrt(I[i]) * rnorm(1)
    }

    return(X)
}

VG.vec <- function(sigma, nu, theta, Time, N, r) {
    h <- Time/N
    Timef <- seq(0, Time, length.out = N + 1)

    I <- rgamma(N, shape = h/nu, scale = nu)  # Vectorized gamma samples
    W <- rnorm(N, mean = 0, sd = 1)  # Pre-generate normal samples
    X <- c(0, cumsum(theta * I + sigma * sqrt(I) * W))  # Efficient cumsum

    return(X)
}

## Create a matrix to fill with random paths using the function VG just created
VG_paths <- matrix(nrow = nsim, ncol = N + 1)
for (i in 1:nsim) {
    VG_paths[i, ] <- VG(sigma, nu, theta, Time = Time, N, r)
}

t(VG_paths) %>%
    format(scientific = TRUE, digits = 2) %>%
    datatable()

# plot the Monte Carlo Simulation
colori <- viridis(nsim)
plot(VG_paths[1, ], col = 0, type = "l", ylim = c(min(VG_paths), max(VG_paths)),
    main = "Monte Carlo Simulation for VG returns", sub = paste(N, "steps", nsim,
        "paths"), xlab = "Time", ylab = "VG returns")
for (i in 2:nsim) {
    lines(VG_paths[i, ], col = colori[i], lwd = 2)
}


lims <- c(min(VG_paths), max(VG_paths))

grid.arrange(ncol = 2, top = "Monte Carlo Simulation for Variance Gamma returns",
    VG_paths %>%
        t() %>%
        quickplot(subtitle = paste("All", nsim, "simulations"), show_legend = F) +
        ylim(lims), quickplot(apply(VG_paths, 2, mean), subtitle = "Mean across timesteps",
        show_legend = F) + ylim(lims))
```

## Testing basic levy

following the scheme to option pricing we do test of fitting according to qqplot and the density poligon,

```{r levy process test of fitting}
## TESTS (both graphical and not) OF DISTRIBUTIONAL ASSUMPTIONS QQplot
l_ret.s <- sort(as.numeric(l_ret))  # sort the log returns

p <- ppoints(length(l_ret.s))  # plotting position

VG.q <- qvg(p, vgC = c, sigma = sigma, theta = theta, nu = nu)  # compute the quantile

plot(x = VG.q, y = l_ret.s, main = "Variance-Gamma Q-Q Plot", xlab = "Theoretical Quantiles",
    ylab = "Sample Quantiles")
abline(0, 1, col = "red")  # Add reference line

ggplot(data.frame(VG.q), aes(sample = as.numeric(VG.q))) + stat_qq(color = "blue") +
    stat_qq_line(color = "black", size = 1) + labs(title = "QQ Plot of VG", x = "Theoretical Quantiles",
    y = "Sample Quantiles") + theme(plot.subtitle = element_text(hjust = 0.5))

# good result, linear

plot_density_comparison <- function(data, title = "Density Comparison", kde_color = "coral2", vg_color = "seagreen3") {
  # Compute Kernel Density Estimate (KDE)
  kde_data <- density(data)
  kde_df <- data.frame(x = kde_data$x, y = kde_data$y)

  # Compute Variance Gamma (VG) density
  x_vals <- seq(min(data), max(data), length.out = 500)
  vg_y_vals <- dvg(x_vals, mean(data), sd(data))
  vg_df <- data.frame(x = x_vals, y = vg_y_vals)

  # Plot using ggplot2
  ggplot() +
    geom_line(data = kde_df, aes(x = x, y = y, color = "Kernel Density"), linewidth = 1) +
    geom_line(data = vg_df, aes(x = x, y = y, color = "Variance Gamma"), linewidth = 1) +
    scale_color_manual(values = c("Kernel Density" = kde_color, "Variance Gamma" = vg_color)) +
    labs(x = "", y = "", title = title, color = "Density Type") +
    theme_minimal() 
}

# Example usage
plot_density_comparison(l_ret, title = "My Custom Density Plot")
```

H0 = The data is consistent with a specified reference distribution. H1 = The data is NOT consistent with a specified reference distribution

AIC doesnt use $c / vgC$ as paramter

[Commmon question: Why do we use the Kolmogorov-Smirnov test?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br> assumption of normality

```{r testing levy process}
# Chi^2 test
test <- chisq.test(l_ret.s, VG.q)
test

paste("With a Chi^2 test: high p-value (0.24)")
ifelse(test$p.value < 0.24,
  paste("We can't reject the null hypotesis as the p-value is", round(test$p.value, 4)),
  paste("We reject the null hypotesis as the p-value is", round(test, 4))
)

# K-S test
test <- ks.test(as.numeric(l_ret), rvg(length(as.numeric(l_ret)), param = c(c, sigma, theta, nu)))
test

paste("With a Kolmogorov-Smirnov test: high p-value (0.10)")
ifelse(test$p.value < 0.10,
  paste("We can't reject the null hypotesis as the p-value is", round(test$p.value, 4)),
  paste("We reject the null hypotesis as the p-value is", round(test$p.value, 4))
)

# Summary statistics through basicStats from fbasics
final_retVG <- VG_paths[, N + 1]

# desc_df(data.frame(final_retVG))
data.frame(t(basicStats(final_retVG)))

gghistogram(final_retVG, bins = 30, title = "Histogram of last time steps", 
            subtitle = paste0("not much disclosing when nsim is small (now = ", nsim,")"))
```

## From variance-gamma returns to stock prices

Before actually thinking of doing option pricing with Levy processes, there are two transformations that need to be performed on our chosen Levy process:

The exponential transformation (because our Levy process starts from zero and could also go below zero): this is necessary for the Levy processes to be able to represent the underlying asset;

Pass from the physical process to the risk neutral one: this is necessary in order to price the option without arbitrage.

So in the end it becomes:

$$S(t) = S(0)e^{rt+Z_t}$$

$Z_t$ levy process \* some people add the compounding factor of $rt$ we use it starting from $S_0$ which is the last price of the timeseries available

```{r}
r <- 0.01
S0 <- as.numeric(tail(S, n=1)) #prezzo inziale
S0

# function for stock price with VG returns
VGexp <- function(sigma, nu, theta, Time, N, r, S0) {
  a <- 1 / nu
  b <- 1 / nu
  h <- Time / N
  Time <- (0:N) * Time / N
  X <- rep(0, N + 1)
  I <- rep(0, N)
  X[1] <- S0
  for (i in 1:N) {
    I[i] <- rgamma(1, a * h, b) # gamma component for the jump
    X[i + 1] <- X[i] * exp(r * Time + theta * I[i] + sigma * sqrt(I[i]) * rnorm(1))
  }
  return(X)
}

VGexp_paths <- matrix(nrow = nsim, ncol = N + 1)
for (i in 1:nsim) {
  VGexp_paths[i, ] <- VGexp(sigma, nu, theta, Time, N, r, S0)
}

VGexp_paths %>% t() %>% round(4) %>%  datatable()

plot(VGexp_paths[1,], col=0, type="l", ylim = c(min(VGexp_paths),max(VGexp_paths)),
     main = "MC Simlation for VG stock prices", sub = "100 steps, 10 paths",
     xlab = "Time", ylab = "S&P 500")
for(i in 2:nsim){
  lines(VGexp_paths[i,], col=colori[i], lwd = 2);

}

lims <- c(min(VGexp_paths), max(VGexp_paths))

grid.arrange(
  ncol = 2, top = "Monte Carlo Simulation for Variance Gamma stock Levy process", 
  quickplot(t(VGexp_paths), title = "", subtitle = paste("All",nsim, "simulations"), show_legend = F)+
    ylim(lims),
  quickplot(apply(VGexp_paths, 2, mean), title = "Mean across timesteps", show_legend = F)+
    ylim(lims)
  )


## Statistics on final prices
final_pricesVG<-VGexp_paths[,N+1]

# Risk neutral transform MCM
rn_final_pricesVG<-S0*final_pricesVG*(exp(r*Time)/mean(final_pricesVG))

# (S0*VGexp_paths*(exp(r*Time)/apply(VGexp_paths,2, mean))) %>% 
#   quickplot("Mean correcting martingale of the whole VG stock paths", show_legend = F)  ## dont think its correct

# taking all the last 5 gridpoints to see how they change in the last time steps
basicStats(data.frame(
  VGexp_paths[, N - 4],
  VGexp_paths[, N - 3],
  VGexp_paths[, N - 2],
  VGexp_paths[, N - 1],
  VGexp_paths[, N],
  VGexp_paths[, N + 1])) %>% 
  set_colnames(c("T-5", "T-4", "T-3", "T-2", "T-1", "T")) %>% 
  round(4) %>%
  mutate(Statistics = rownames(basicStats(1)))%>%   # just to extract the rownames 
  relocate(Statistics, .before = everything())  %>% # move this new column
  gt() %>% 
  data_color(alpha = 0.2, columns = 2:7, rows = everything(), autocolor_text = F, direction = "row", 
             palette = c("red", "white", "green")) %>% 
  opt_stylize(5, "gray", F) %>%
  tab_options(table.font.size = px(11)) %>% 
  tab_header("Variance gamma model last stock stock prices statistics")


grid.arrange(top = "Distribution of last prices", ncol=2,
gghistogram(rn_final_pricesVG, bins = 30)+ labs(subtitle = "After MCM"),
gghistogram(final_pricesVG, bins = 30, fill = "firebrick")+ labs(subtitle = "before MCM"))

##OPTION PRICING
K <- S0 #prova: opzione ATM
payoff_VG <- pmax(rn_final_pricesVG - K, 0)
optprice_VG <- mean(payoff_VG)*exp(-r*Time)
optprice_VG
```

### Pricing with FFT under the VG process

```{r VG FFT}
# VG process
theta <- -0.1436
nu <- 0.3
r <- 0.1
sigma <- 0.12136
Time <- 1/4
K <- 101
S <- 100
alpha <- 1.65

phiVG <- function(u) {
    omega <- (1/nu) * (log(1 - theta * nu - sigma^2 * nu/2))
    tmp <- 1 - (0 + (0+1i)) * theta * nu * u + 0.5 * sigma^2 * u^2 * nu
    tmp <- tmp^(-1/nu)
    exp((0 + (0+1i)) * u * log(S0) + u * (r + omega) * (0 + (0+1i))) * tmp
}

FFTcall.price <- function(phi, S0, K, r, Time, alpha = 1, N = 2^12, eta = 0.25) {
    m <- r - log(phi(-(0 + (0+1i))))
    phi.tilde <- function(u) (phi(u) * exp((0 + (0+1i)) * u * m))^Time
    psi <- function(v) {
        exp(-r * Time) * phi.tilde((v - (alpha + 1) * (0 + (0+1i))))/(alpha^2 + alpha -
            v^2 + (0 + (0+1i)) * (2 * alpha + 1) * v)
    }
    lambda <- (2 * pi)/(N * eta)
    b <- 1/2 * N * lambda
    ku <- -b + lambda * (0:(N - 1))
    v <- eta * (0:(N - 1))
    tmp <- exp((0 + (0+1i)) * b * v) * psi(v) * eta * (3 + (-1)^(1:N) - ((1:N) -
        1 == 0))/3
    ft <- fft(tmp)
    res <- exp(-alpha * ku) * ft/pi
    inter <- spline(ku, Re(res), xout = log(K/S0))
    return(inter$y * S0)
}

FFTcall.price(phiVG, S0 = S0, K = K, r = r, Time = Time)
black_scholes(S = S0, K = S0, Time = Time, r = r, sigma = sigma, type = "call")
```

## Meixner model

jump components driven now by the meixner distribution:

-   a (scale)
-   b (skewness)
-   d (tail heaviness)
-   m (location)

[Commmon question: How to estimate Meixner model?]{style="color: darkorange; text-decoration: underline; font-weight: bold;"} <br> 4 components but levy never a perfect martingale density function is too convoluted so i can't perform MLE since the resutls will not be stable, so i have to settle for MoM

Lévy Process: Independent/stationary increments, starts at zero.

Markovian: Future values depend only on the current state.

Submartingale Behavior: Typically drifts upward due to dominant positive jumps (though drift-jump balance can vary).

**Properties**

It has independent and stationary increments, and starts at zero.

Markovian: Like all Lévy processes, the Meixner process is Markovian—future values depend only on the current state, not on the past trajectory.

Submartingale Behavior: In practice, the jump component often dominates the drift. Since the negative drift is usually smaller in magnitude than the positive jumps, the process tends to behave as a submartingale (i.e., it drifts upward onaverage), although this can vary depending on the data

since we are using MoM you get less efficient estimates than MLE since MoM ignores full distributional information, Cannot use AIC/BIC for model comparison (no likelihood function).

$$
f(x; a, b, d, m) = \frac{(2 \cos(b/2))^{2d}}{2a\pi\Gamma(2d)} 
\exp\left( \frac{b(x - m)}{a} \right) 
\Gamma\left( d + \frac{i(x - m)}{a} \right)^{-1}
$$

where $a>0$, $-\pi<b<\pi$, $d>0$, and $m \in \mathbb{R}$.

```{r Meixner model MoM}
# Moments: mean, variance, skewness, kurtosis
x <- mean(l_ret, na.rm = TRUE)
y <- sd(l_ret, na.rm = TRUE)
z <- as.numeric(skewness(l_ret, na.rm = TRUE))
w <- as.numeric(kurtosis(l_ret, na.rm = TRUE))

# Mom: estimates parameters m, a, b, d as functions of the moments
m <- x - ((z * sqrt(y)) / (w - (z^2) - 3))
a <- sqrt(y * (2 * w - 3 * (z^2) - 6))
b <- 2 * atan(-sqrt((z^2) / (2 * w - 3 * (z^2) - 6)))
d <- 1 / (w - (z^2) - 3)

# risk neutral transformation
# Esscher transform: Meixner(a, a*theta + b, d, m) distribution

# theta <- -1/a * (b + 2 * atan((-cos(a/2)+ exp((m-r)/2*d))/sin(a/2)))
# b <- a*theta+b

# mean correction

# m <- r -2 *d*log(cos(b/2)/cos((a+b)/2))

data.frame(mean=x, sd=y, skew=z, kurt=w)
data.frame(a=a, b=b, d=d, m=m)
```

```{r Meixner model}
# Meixner function
MX <- function(a, b, d, M, N) {
  distr <- udmeixner(a, b, d, m)   # meixner distribution
  gen <- pinvd.new(distr)          # Polynomial interpolation of INVerse CDF
  rdmMXgen <- ur(gen, N)           # randomly draws N objects from gen (from a Meixner distr)
  h <- Time / N
  X <- rep(0, N + 1)
  for (i in 1:N) {
    X[i + 1] <- X[1] + rdmMXgen[i]
  }
  return(X)
}

replicate(10,MX(a, b, d, m, N)) %>% quickplot()

MX_paths <- matrix(nrow = nsim, ncol = N + 1) # fill the matrix with random paths that follow
for (i in 1:nsim) { 
  MX_paths[i, ] <- MX(a, b, d, m, N)
}

MX_paths %>%
  t() %>%
  format(x = , scientific = TRUE, digits = 3) %>%
  datatable()

# plot the Monte Carlo Simulation
# plot(MX_paths[1, ],
#   col = 0, type = "l", ylim = c(min(MX_paths), max(MX_paths)),
#   main = "Monte Carlo Simulation for Meixner returns", sub = "100 steps, 10 paths",
#   xlab = "Time", ylab = "MXNR returns"
# )
# for (i in 2:nsim) {
#   lines(MX_paths[i, ], col = colori[i], lwd = 2)
# }

t(MX_paths)[1:10,] %>% quickplot(show_legend = F)

lims <- c(min(MX_paths), max(MX_paths))

grid.arrange(
  ncol = 2, top = "Monte Carlo Simulation for Meixner model Returns Levy process", 
  quickplot(t(MX_paths), title = "", subtitle = paste("All",nsim, "simulations"), show_legend = F)+
    ylim(lims),
  quickplot(apply(MX_paths, 2, mean), title = "Mean across timesteps", show_legend = F)+
    ylim(lims)
  )

# QQplot
MX.q <- uq(pinvd.new(udmeixner(a, b, d, m)), p) # compute the quantile

plot(MX.q, l_ret.s,
  main = "Meixner Q-Q Plot",
  xlab = "Theoretical Quantiles", ylab = "Sample Quantiles"
)
# good result, linear

# summary statistics
final_retMX <- MX_paths[, N + 1]
basicStats(final_retMX)
gghistogram(final_retMX, bins = 30, title = "Histogram of last time steps", 
            subtitle = paste0("not much disclosing when nsim is small (now = ", nsim,")"))
```

Heavy-tailed and skewed, making it ideal for modeling financial returns (captures volatility clustering and extreme events).

### Meixner model stock prices

apply the same $S_0 e^{rtMX}$ to get the stock prices

```{r Meixner model stock prices}
#FROM MEIXNER RETURNS TO STOCK PRICES

#function for stock price with Meixner returns
MXexp=function(a, b, d, m, N, Time, r, S0) {
  distr <- udmeixner(a, b, d, m)     # meiner distribution
  gen <- pinvd.new(distr)            # Polynomial interpolation of INVerse CDF
  generazioni <- ur(gen,N)           # randomly draws N objects from gen (from a Meixner distr)
  h=Time/N
  Time=(0:N)*Time/N
  X=rep(0,N+1)
  X[1]=S0
  for (i in 1:N){
    X[i+1]=X[1]*exp(r*Time+generazioni[i])
  }
  return(X)
}

replicate(10,MXexp(a, b, d, m, N, Time, r, S0)) %>% quickplot()

MXexp_paths<-matrix(nrow = nsim, ncol=N+1)
for(i in 1:nsim){
  MXexp_paths[i,]<-MXexp(a,b,d,m,100,Time,r,S0) #vengono tutte le linee uguali perché MX non varia!
}

MXexp_paths %>% t() %>% round(2) %>% datatable()

lims <- c(min(MXexp_paths), max(MXexp_paths))

grid.arrange(
  ncol = 2, top = "Monte Carlo Simulation for Meixner exponential stock Levy process", 
  quickplot(t(MXexp_paths), title = "", subtitle = paste("All",nsim, "simulations"), show_legend = F)+
    ylim(lims),
  quickplot(apply(MXexp_paths, 2, mean), title = "Mean across timesteps", show_legend = F)+
    ylim(lims)
  )

#statistics on final prices
final_pricesMX<-MXexp_paths[,N+1]

basicStats(data.frame(
  MXexp_paths[, N - 4],
  MXexp_paths[, N - 3],
  MXexp_paths[, N - 2],
  MXexp_paths[, N - 1],
  MXexp_paths[, N],
  MXexp_paths[, N + 1])) %>% 
  set_colnames(c("T-5", "T-4", "T-3", "T-2", "T-1", "T")) %>% 
  round(4) %>%
  mutate(Statistics = rownames(basicStats(1)))%>%   # just to extract the rownames 
  relocate(Statistics, .before = everything())  %>% # move this new column
  gt() %>% 
  data_color(alpha = 0.2, columns = 2:7, rows = everything(), autocolor_text = F, direction = "row", 
             palette = c("red", "white", "green")) %>% 
  opt_stylize(5, "gray", F) %>%
  tab_options(table.font.size = px(11)) %>% 
  tab_header("Meixner model last stock stock prices statistics")



#risk neutral transform
rn_final_pricesMX<-S0*(final_pricesMX)*(exp(r*Time)/(mean(final_pricesMX)))
rn_final_pricesMX %>% t() %>% datatable()

basicStats(rn_final_pricesMX)
hist(rn_final_pricesMX)

########payoff if you use the Esscher transform or the first mean correcting martingale

#payoff_MX <- pmax(final_pricesMX - K, 0)

#optprice_MX <- mean(payoff_MX)*exp(-r*Time)

#optprice_MX

#########payoff if you mean correct of the simulated paths

payoff_MX <- pmax(rn_final_pricesMX - K, 0)

optprice_MX <- mean(payoff_MX)*exp(-r*Time)

optprice_MX
```

# American options

higher price since its evalued at all times and even OTM can become ITM with time and volatility using Longstaff and Schwartz method

This method is extremely easy to understand and implement. Remember that the objective is to estimate the value

$$
C = \max_τ \mathbb E\left[e−rτ max(S − k, 0)\right]
$$

Over all possible sets of times τ ≤ T. In practice this is only a finite set of times on a grid as all simulation schemes are by their nature discrete.

**Core Challenges of American Options Pricing**

Early Exercise: Unlike European options, American options can be exercised at any time before expiration, requiring dynamic optimization.

Curse of Dimensionality: Traditional methods (e.g., binomial trees) become computationally infeasible for high-dimensional problems due to exponential growth of paths (e.g., 100\^3 simulations for 3 time steps).

Longstaff-Schwartz Method (LSM) Key Idea: Combines Monte Carlo simulation with backward induction and regression to estimate the optimal exercise strategy, get the payoff and pricing based on that .

Steps:

-   Simulate Paths: Generate $N$ price paths under the risk-neutral measure.

-   Backward Induction: Start at expiration (t=T) and compute the immediate payoff.

Move backward to t=T−1:

Compare the immediate exercise value vs. the continuation value (estimated via regression on future discounted payoffs).

Decision: **Exercise if immediate payoff \> continuation value.**

Repeat until t=0.

-   Regression:

Fit a model (e.g., polynomial) to discounted future payoffs using in-the-money (ITM) paths.

Basis functions: $X,X^2,X^3$ (flexible choice).

-   Discount & Average:

Discount cash flows from optimal exercise times to t=0 and average across all paths.

```{r}
## least squares method
LSM <- function(n, d, S0, K, sigma, r, Time) {
  s0 <- S0 / K
  dt <- Time / d
  z <- rnorm(n)
  s.Time <-                  s0 * exp((r - 1 / 2 * sigma^2) * Time + sigma * z * (Time^0.5))
  s.Time[(n + 1):(2 * n)] <- s0 * exp((r - 1 / 2 * sigma^2) * Time - sigma * z * (Time^0.5))
  CC <- pmax(1 - s.Time, 0)
  payoffeu <- exp(-r * Time) * (CC[1:n] + CC[(n + 1):(2 * n)]) / 2 * K
  euprice <- mean(payoffeu)
  
  for (k in (d - 1):1) {
    z <- rnorm(n)
    mean <- (log(s0) + k * log(s.Time[1:n])) / (k + 1)
    vol <- (k * dt / (k + 1))^0.5 * z
    s.Time.1 <- exp(mean + sigma * vol)
    mean <- (log(s0) + k * log(s.Time[(n + 1):(2 * n)])) / (k + 1)
    s.Time.1[(n + 1):(2 * n)] <- exp(mean - sigma * vol)
    CE <- pmax(1 - s.Time.1, 0)
    idx <- (1:(2 * n))[CE > 0]
    discountedCC <- CC[idx] * exp(-r * dt)
    
    ## three basis function x^0 x^1 and x^2
    basis1 <- exp(-s.Time.1[idx] / 2)
    basis2 <- basis1 * (1 - s.Time.1[idx])
    basis3 <- basis1 * (1 - 2 * s.Time.1[idx] + (s.Time.1[idx]^2) / 2)
    
    
    p <- lm(discountedCC ~ basis1 + basis2 + basis3)$coefficients
    estimatedCC <- p[1] + p[2] * basis1 + p[3] * basis2 + p[4] * basis3  ## cont region
    EF <- rep(0, 2 * n)
    EF[idx] <- (CE[idx] > estimatedCC)                                   ## compare the payoff with cont 
    CC <- (EF == 0) * CC * exp(-r * dt) + (EF == 1) * CE
    s.Time <- s.Time.1
    
    # print(z)
    # print(mean)
    # print(vol)
    # print(s.Time.1)
    # print(mean)
    # print(s.Time.1)
    # print(CE)
    # print(idx)
    # print(discountedCC)
    # print(basis1)
    # print(basis2)
    # print(basis3)
    # print(p)
    # print(estimatedCC)
    # print(EF)
    # print(EF)
    # print(CC)
    # print(s.Time)
  }

  payoff <- exp(-r * dt) * (CC[1:n] + CC[(n + 1):(2 * n)]) / 2
  usprice <- mean(payoff * K)
  error <- 1.96 * sd(payoff * K) / sqrt(n)
  earlyex <- usprice - euprice
  data.frame(usprice, error, euprice)
}
S0 <- 135
K <- 135
Time <- 1
r <- 0.05
sigma <- 0.4
LSM(1000000, 3, S0, K, sigma, r, Time)
```

| **Method** | **Pros** | **Cons** |
|------------------------|-------------------------|------------------------|
| Binomial Tree | Exact for 1D, handles early exercise. | Exponential complexity in high dimensions. |
| Finite Difference | PDE-based, precise for low dimensions. | Hard to extend beyond 2-3 dimensions. |
| LSM | Handles high dimensions, easy to implement. | Approximation error, depends on regression. |

# Fraone

```{r, include=FALSE}
knitr::opts_chunk$set(eval = T)  # Resume execution
```

## Black-Scholes model: Moving beyond the normality of returns

A key assumption of the Black-Scholes model is that the underlying asset follows a Geometric Brownian Motion process. This implies that returns are normally distributed and, in turn, the underlying price is log-normally distributed.

```{r}
grid.arrange(
  ncol = 2,
  quickplot(dnorm((-400:400) / 100), title = "Theoretical return distribution", show_legend = F, xlab = NULL, ylab = NULL, x_size = 100, x_start = -4),
  quickplot(dlnorm(seq(0.01, 5, length.out = 500), meanlog = 0, sdlog = 0.7), title = "Theoretical underlying price distribution", show_legend = F, xlab = NULL, ylab = NULL, x_size = 100, x_start = 0)
)
```

From a logical standpoint, pricing an option is all about correctly assessing the probability of the option to end up in the money (St\>K) and correctly quantifying the expected value of the future payoff.

The complexity of option pricing is not the evaluation of the expected value of the payoff itself. In fact, the crucial part is the assumption regarding the future distribution of the underlying asset. If we get this wrong, then our pricing will be incorrect.

```{r}
x <- dlnorm(seq(0.01, 5, length.out = 500), meanlog = 0, sdlog = 0.7)

quickplot(x, title = "Theoretical underlying price distribution", xlab = NULL, ylab = NULL, x_size = 100, x_start = 0) +
  geom_vline(aes(xintercept = 2)) +
  geom_ribbon(aes(xmin = 0, xmax = 2, color = NULL, fill = "OTM"), alpha = 0.1) +
  geom_ribbon(aes(xmin = 2, xmax = 5, color = NULL, fill = "ITM"), alpha = 0.1) +
  scale_fill_manual("Moneyness", values = c(OTM = "lightblue", ITM = "indianred")) +
  guides(color = "none")
```

Option pricing is all about making an assumption regarding the future probability distribution of the underlying. This assumption mainly depends on two factors: the type of distribution chosen and its parameters.

Under Black-Scholes, the type of distribution of the underlying returns is the normal distribution. Under the assumption of normal distribution, the only parameter required is the standard deviation of returns (σ). This is an input to the Black-Scholes model.

When you estimate σ, regardless of the method you use (e.g. Historical, MLE, Quasi￾MLE, etc.), you are implicitely assuming that the calibrated σ will be a good representation of the standard deviation of returns in the future (i.e. the life of the option).

**At this point it is fair to wonder whether the assumption of normality for the underlying returns is appropriate.**

```{r include=FALSE}
fraoneshow <- function(ticker, from, to, title = NA) {
  realtitle <- ifelse(is.na(title), yes = paste("Time difference of", to - from, "days, between", from, "and", to), no = title)
  df <- ROC(Cl(getSymbols(ticker, from = from, to = to, auto.assign = F)), type = "cont")

  suppressWarnings(grid.arrange(
    ncol = 2, top = realtitle,
    gghistogram(df, add.rug = F, title = NULL) +
      labs(subtitle = paste("Histogram of logreturns of", ticker)) +
      theme(plot.subtitle = element_text(hjust = 0.5))
    
    ,
    
    ggplot(df, aes(sample = as.numeric(df))) +
      stat_qq(color = "blue") +
      stat_qq_line(color = "black", size = 1) +
      labs(subtitle = paste("QQ Plot of", ticker), x = "Theoretical Quantiles", y = "Sample Quantiles") +
      theme(plot.subtitle = element_text(hjust = 0.5))))
}
```

### Stock indeces

We start with the main equity indexes, namely the S&P500, Nasdaq and Dow Jones. Specifically, we will be using the most traded ETFs for these indexes: SPY, QQQ and DIA respectively. The results would be virtually identical if we used the future contracts for each index instead of their ETFs.

```{r}
fraoneshow("SPY", from = Qdate(1,1,2018), to = Qdate(1,1,2024))
fraoneshow("QQQ", from = Qdate(1,1,2018), to = Qdate(1,1,2024))
fraoneshow("DIA", from = Qdate(1,1,2018), to = Qdate(1,1,2024))
```

The three indexes analysed above offer an aggregate view on the whole equity market, however they are comprised of several stocks from different sectors. While it is interesting to observe the empirical behaviour of the entire market, it can’t be ignored that each sector and each stock within a sector have their own behaviour. Although the empirical distribution of returns will not vary majorly across sectors or individual stocks, it is important to remember that there are differences.

As an example, we have extended the analysis done on the three equity indexes to the ETFs of some major equity sectors, specifically:

$\sigma$ is important the price goes up the stairs and goes down by the elevator

A pronounced left tail means that the BS fail since your empirical distribution changes heavily in that point which is important for pricing

-   Financials (XLF - Financial Select Sector SPDR)

-   Consumer staples (XLP - Consumer Staples Select Sector SPDR).

```{r}
fraoneshow("XLF", from = Qdate(1,1,2018), to = Qdate(1,1,2021))
fraoneshow("SLP", from = Qdate(1,1,2018), to = Qdate(1,1,2021))
```

### Bonds

Similarly, we’ll now extend the analysis beyond equities and we’ll move into different asset classes. The same analysis is presented here below regarding the bond market. For the sake of this example we are looking at the TLT ETF (iShares 20+ Year Treasury Bond ETF) which is a very liquid and actively traded ETF on the US bond market.

```{r}
fraoneshow("TLT", from = Qdate(1,1,2018), to = Qdate(1,1,2021))
```

### Single stocks

Earlier we have observed the distributions of equity indices and sector ETFs, which represent a broad basket of stocks. This provides a view on the behaviour of the overall market, however each stock has unique features. Below we will look at the empirical distributions of some individual stocks, from different sectors. For the purpose of this analysis, we will look at the following:

-   GME and AMC, two of the so called ‘meme stocks’, which became popular in 2021, when they recorded major spikes following short squeezes

-   CCL and CSCO, from the space of leisure and telecom sectors

-   SE and Z, two tech stocks which have gained popularity from 2020 onwards

#### Gamestop

massive difference in movment between the 2021 movments with the wallstreetbets subreddit pushing prices up and the same period in 2020

```{r}
fraoneshow("GME", from = Qdate(1,1,2020), to = Qdate(1,6,2020))
fraoneshow("GME", from = Qdate(1,1,2021), to = Qdate(1,6,2021))
```

#### AMC Entertainment

```{r}
fraoneshow("AMC", from = Qdate(1,1,2020), to = Qdate(1,6,2020))
fraoneshow("AMC", from = Qdate(1,1,2021), to = Qdate(1,6,2021))
```

#### CCL carrnival cruises

During covid they couldnt do anything and had massive mantainance costs, immediately think

```{r}
fraoneshow("CCL", from = Qdate(1,1,2018), to = Qdate(1,6,2019))
fraoneshow("CCL", from = Qdate(1,1,2020), to = Qdate(1,6,2021))
```

#### Zeta

single tech stock with a couple of bull runs happening quickly

```{r}
fraoneshow("Z", from = Qdate(1,1,2018), to = Qdate(1,1,2021))
```

We have observed the features of the empirical distributions of several assets, from different asset classes. All these show some evidence of non-normality of returns, as we have discussed. **There is also another feature to bear in mind. What observed in the charts so far is not static.** Any financial asset may display a changing behaviour, based on the period we are looking at. As observed so far, assuming normality of returns may lead to inaccurate pricing. The degree of such inaccuracy may be different based on the period we are looking at. In order to provide an example, we will now look at the distributions of two of the assets analysed earlier, after restricting the period of the time series. The charts will highlight the differences in the empirical distributions compared to the ones of the longer period analysed earlier.

### Commodities

Commodities offer different insights regarding the empirical distribution Moving to commodities, the same analysis is presented here below for gold. We are looking at the GLD ETF (SPDR Gold Trust ETF) which is one of the most traded ETF on gold.

```{r}
fraoneshow("GLD", from = Qdate(1,1,2018), to = Qdate(1,1,2021))
```

### OIL

Looking at energy commodities, we have analysed a major Oil ETF (USO) and a future contract on TTF natural gas, which is the main gas market in Europe. We will shortly see that the thesis of non normality of returns is confirmed in this case as well, although these two commodities offer different insights regarding the empirical distribution. In particular, it is interesting to observe the behaviour of returns for these commodities, which are very different from all other assets analysed so far. This reflects the nature of these markets which are strongly driven by their fundamental supply/demand dynamics. **Oil shows climate changes like cold spells and wars on the right tail supply and demand dynamics seen in the tails**

```{r}
fraoneshow("USO", from = Qdate(1,1,2020), to = Qdate(1,6,2020))
fraoneshow("USO", from = Qdate(1,1,2021), to = Qdate(1,6,2021))
```

We have analysed the empirical distribution of returns for a number of instruments across different asset classes and we have confirmed that it is rare to find normally distributed returns.

What are the implications in terms of option pricing?

Think about what we discussed earlier and the importance of the assumption of the underlying returns distribution. In short, pricing under the Black-Scholes model, which implicitely assumes normality of returns, may lead to inaccurate option pricing! Does it mean that Black-Scholes should not be used? Absolutely not! Despite its limitations, Black-Scholes offers a number of advantages, such as mathematical tractability, ease of use and a closed formula solution for pricing and greeks.

In fact, Black-Scholes is still the most popular model used for option pricing in the industry.

### Conclusions

Drop the assumption of normality of returns (i.e. Geometric brownian motion)and use more advanced stochastic processes instead. Later in the course you will study a number of different stochastic processes, including Lévy processes. Such processes typically have more parameters and allow to better represent kurtosis and skewness observed in the empirical distributions of returns.

Continue using the Black-Scholes model, although with the introduction of some adjustments to the σ used as input, as normally done by market participants in the industry. As a result of this process, in practice we will observe the phenomenon of the volatility smile (also known as volatility skew).

## Black-Scholes model: Implied volatility

### Implied volatilty

volatilty implied by the market price the market is always right instead of seeing options as $P=f(S,K,r,t, \sigma)$ you rewrite the function as $\sigma=f(S,K,r,t,P)$, you cant do it analytically and so you have to do this numerically options traded respect believed prices and and all market operators use the same model and the same GbM you should always see the same value coming out of the procedure

in reality you dont see this and the option price should be the one that sigma will not be a function of anything else

when you do this with market prices and you get the implied volatility you will not have the flat shape of the implied volatility, so returns are not normal and the GBM is not the correct way to visualize prices as that is the only parameter to change as the rest is observable and fixed

## Black-Scholes model: Volatility smile

backing out the volatilty as a function of the price on the market not possible to do in a closed form solution

we can still solve it numerically and obtain the value of $σ$, which is typically called implied volatility, given that it is the volatility implied by real market option prices.

If you repeat this process for all the available strike prices for the same option, you will realise that the value of the implied volatility that you obtain changes every time.

In theory, if the Black-Scholes assumption of normality of returns was correct, **we should get always the same σ, regardless of the strike price or underlying price.** This is simply due to the model specification of Black-Scholes. The parameter $σ$ is meant to be a constant in the Black-Scholes model, not a function of the strike price or underlying price.

However, if we get a different σ depending on the strike or underlying price of the option, this implies that σ is not a constant

```{r}
paste("python files")
```

Black-Scholes pricing function<br>
`def BS_option_price(S, K, T, r, sigma, option_type):`<br>
`    d1 = (np.log(S/K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))`<br>
`    d2 = d1 - sigma * np.sqrt(T)`<br>
<br>
`    if option_type == 'call':`<br>
`        option_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)`<br>
`    elif option_type == 'put':`<br>
`        option_price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)`<br>
`    else:`<br>
`        raise ValueError("Invalid option type. Please define 'call' or 'put'.")`<br>
`    return option_price`<br>

Implied volatility function

scipy is a library that allows for a lot of statistical analysis 

minimize the objective functions

` def implied_volatility(option_mkt_price, S, K, T, r, option_type, initial_guess):`<br>
`    result = minimize(objective_function, initial_guess, args=(option_mkt_price, S, K, T, r, option_type), method='Nelder-Mead') `<br>
`    implied_vol = result.x ## only take the first element, python is 0 index`<br>
`    return implied_vol`<br>
    

inside implied volatility you need the objective function as an argument 

Objective function<br>
`def objective_function(sigma, option_mkt_price, S, K, T, r, option_type):`<br>
`    return abs((BS_option_price(S, K, T, r, sigma, option_type) - option_mkt_price))`<br>


```{r}
library(readxl)
Options_input <- read_excel("~/aCATTOLICA ANNO 24-25/2 SEMESTRE/(ASF) - Applied Statistics for Finance/(ASF) - Python/Options_input.xlsx")
Options_input
```

Add an empty column in df_option_data (we will store the implied volatility values in this column later)

`df_option_data['Implied vol'] = float('nan')`<br>

Loop through df_option_data rows, compute the implied volatility for each option and store the value in the 'Implied vol' column of the dataframe

using a for loop 

`for opt in range(df_option_data.shape[0]):`<br>
`    df_option_data['Implied vol'][opt] = implied_volatility(option_mkt_price = df_option_data['Option price'][opt],`<br>
`                                                            S=S0, r=r, T=T,`<br>
`                                                            initial_guess= initial_guess,`<br>
`                                                            K=df_option_data['Strike price'][opt], `<br>
`                                                            option_type=df_option_data['Option type'][opt])`<br>


or using lambda functions:

`df_option_data['Implied vol'] = df_option_data.apply(lambda row: implied_volatility(row['Option price'],`<br>
`                                                       S0, row['Strike price'], T, r,`<br>
`                                                       row['Option type'], initial_guess), axis=1)`<br>


## The Volatility Index (VIX)

## Option pricing and Liquidity

An important assumption widely made in option pricing is perfect liquidity. This assumption is generally made for any pricing model, not only for the Black-Scholes model

In practice any asset typically has two prices: the bid price and the ask price

And 2 things happen, either the prices are close enough that it doesnt make a difference or it changes things

you can do both bid-ask prices since the "reliable correct price" is somewhere in the middle

take the midpoint as a proxy

or take the last price traded if its sufficiently recent

and since liquidity of the option market is much lower than the liquidity of the market for the underlying you really feel it

option chains hide the liquidity since most of it is going to be at the ATM points

time also plays a major role, even on the SP500 you go from a BA spread of 5 cents in the 54 days maturity chain to 3-5 dollars BA, all of this at the ATM

lastly you also need to consider the overall size of the market, the smaller market cap stock are going to have way less contracts being traded. Interestingly there is much less spread at deep ITM options than at ATM since people are just putting the fishing line in and waiting. lastly there are no bids for otm options

For stocks like this its best recomended to think in ranges

this chart is for the SPY ETF with volatility smiles visualized with the bid, ask and the midpoint prices, red is puts blue is calls,

![](images/F4 Liquidity Image[17].jpg){fig-align="center" width="598"}

this is a great way to visualize why we use OTM puts and calls to visualize volatility smiles, OTM options are more liquid so there is much less difference ITM calls at the very left are way less liquid and so there is more difference in less liquid

since there is less protection and a more linear relation btw underlying and option price

in general you can see that taking the midpoint is good as it catches pretty well the "good" OTM sides of the volatility skew

## The Greek letters

can be done in closed form with the black and sholes formula since they have been figured out by people before us. they are a linear approximation

or linear approximation of the price changes when varying one variable, provided that you can calculate the price

Greeks: BS closed formula

Calculate the option price via approximation using closed formula greeks

apply a shock to the option input variables (S0, sigma, T, r), calculate the option price after the shock using the greeks approximation, then compare to full option repricing via BS option pricing closed formula ## Delta is this a good approximation of our delta? so give a shock and reprice? reprice using a shock of 1 both positive and can be negative overall a small difference so a linear approximation can be used for small shocks

All of these are fine in linear approximation so it works fine for small changes but when you talk about bigger swings

the question of why to use this when higher order approximations are better? higher calculations effort, also quicker to put it back int the pricing, if delta = 0.4 i know that for every 1 € my position changes by 40 cents for second order its not so quick

Black-Scholes Greeks function
`def BS_greeks(S, K, T, r, sigma, option_type):`<br>
`    d1 = (np.log(S/K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))`<br>
`    d2 = d1 - sigma * np.sqrt(T)`<br>

`    # Delta`<br>
`    if option_type == 'call':`<br>
`        delta = norm.cdf(d1)`<br>
`    elif option_type == 'put':`<br>
`        delta = norm.cdf(d1) - 1`<br>
`    else:`<br>
`        raise ValueError("Invalid option type. Please define 'call' or 'put'.")`<br>

`    # Gamma`<br>
`    gamma = norm.pdf(d1) / (S * sigma * np.sqrt(T))`<br>

`    # Theta`<br>
`    if option_type == 'call':`<br>
`        theta = -((S * norm.pdf(d1) * sigma) / (2 * np.sqrt(T))) - r * K * np.exp(-r * T) * norm.cdf(d2)`<br>
`    elif option_type == 'put':`<br>
`        theta = -((S * norm.pdf(d1) * sigma) / (2 * np.sqrt(T))) + r * K * np.exp(-r * T) * norm.cdf(-d2)`<br>

`    else:`<br>
`        raise ValueError("Invalid option type. Please define 'call' or 'put'.")`<br>

`    # Vega`<br>
`    vega = S * np.sqrt(T) * norm.pdf(d1)`<br>
  
`    # Rho`<br>
`    if option_type == 'call':`<br>
`        rho = K * T * np.exp(-r * T) * norm.cdf(d2)`<br>
`    elif option_type == 'put':`<br>
`        rho = -K * T * np.exp(-r * T) * norm.cdf(-d2)`<br>

`    greeks = {'delta': delta, 'gamma': gamma, 'theta': theta, 'vega': vega, 'rho': rho}`<br>

`    return greeks`

## Greeks: numerical approximation
no longer vanilla options with closed form solutions, now we are off the wall and now we can only do a generalized solution

alternative way is to do a numerical approximation

simply put: recalculating the value of the option and with a positive and negativ shift

in this case its still being priced as BS vanilla option, in any other way you would get the pricing function of the instrument that you are using (so its more of a thought experiment)

```{r}
S0 <- 154.73
timestep <- 1/252
K <- 155
Time <- 33 * timestep
r <- 0.05
sigma <- 0.3018

S0_shock <- 1
# Step 1: calculate the new option price, after a positive shock f(x + h), via full repricing under BS closed formula
optprice_positiveshock <- black_scholes(S0 + S0_shock, K, Time, r, sigma, type = "call")
optprice_positiveshock

# Step 2: calculate the new option price, after a negative shock f(x - h), via full repricing under BS closed formula
optprice_negativeshock <- black_scholes(S0 - S0_shock, K, Time, r, sigma, type = "call")
optprice_negativeshock

# Step 3: calculate the delta via numerical approximation (central difference)
numerical_delta <- (optprice_positiveshock - optprice_negativeshock)/(2 * S0_shock)
numerical_delta
```



